{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07-17-2020_14h16min49s'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%m-%d-%Y_%Hh%Mmin%Ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_epoch_number = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "evo_data = pd.read_csv('data/interpol/evo_interpol_demand.csv', index_col=0)\n",
    "modo_data = pd.read_csv('data/interpol/modo_interpol_demand.csv', index_col=0)\n",
    "c2g_data = pd.read_csv('data/interpol/c2g_interpol_demand.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tempC', 'precipMM', 'FeelsLikeC', 'uvIndex', 'visibility',\n",
       "       'windspeedMiles', 'Blizzard', 'Clear', 'Cloudy', 'Fog', 'Heavy rain',\n",
       "       'Heavy rain at times', 'Heavy snow', 'Light drizzle', 'Light rain',\n",
       "       'Light rain shower', 'Light sleet', 'Light sleet showers', 'Light snow',\n",
       "       'Mist', 'Moderate or heavy freezing rain',\n",
       "       'Moderate or heavy rain shower', 'Moderate or heavy rain with thunder',\n",
       "       'Moderate or heavy sleet', 'Moderate or heavy snow showers',\n",
       "       'Moderate or heavy snow with thunder', 'Moderate rain',\n",
       "       'Moderate rain at times', 'Moderate snow', 'Overcast', 'Partly cloudy',\n",
       "       'Patchy heavy snow', 'Patchy light drizzle', 'Patchy light rain',\n",
       "       'Patchy light rain with thunder', 'Patchy light snow',\n",
       "       'Patchy moderate snow', 'Patchy rain possible', 'Patchy sleet possible',\n",
       "       'Patchy snow possible', 'Sunny', 'Thundery outbreaks possible',\n",
       "       'Torrential rain shower', 'Monday', 'Tuesday', 'Wednesday', 'Thursday',\n",
       "       'Friday', 'Saturday', 'Sunday', 'hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
       "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
       "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
       "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
       "       'hour_23', 'travels', 'interpolate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evo_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_data.drop(columns = ['hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
    "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
    "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
    "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
    "       'hour_23'], inplace=True)\n",
    "modo_data.drop(columns = ['hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
    "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
    "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
    "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
    "       'hour_23'], inplace=True)\n",
    "c2g_data.drop(columns = ['hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
    "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
    "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
    "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
    "       'hour_23'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unievo_data = pd.DataFrame(evo_data.travels)\n",
    "unimodo_data = pd.DataFrame(modo_data.travels)\n",
    "unic2g_data = pd.DataFrame(c2g_data.travels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the Canada Day Holiday\n",
    "evo_data.index = pd.to_datetime(evo_data.index)\n",
    "modo_data.index = pd.to_datetime(modo_data.index)\n",
    "c2g_data.index = pd.to_datetime(c2g_data.index)\n",
    "\n",
    "evo_data[\"holidays\"] = pd.Series()\n",
    "modo_data[\"holidays\"] = pd.Series()\n",
    "c2g_data[\"holidays\"] = pd.Series()\n",
    "\n",
    "evo_data[\"holidays\"] = evo_data[\"holidays\"].fillna(0)\n",
    "modo_data[\"holidays\"] = modo_data[\"holidays\"].fillna(0)\n",
    "c2g_data[\"holidays\"] = c2g_data[\"holidays\"].fillna(0)\n",
    "\n",
    "canada_day = datetime(2018, 7, 1)\n",
    "end_canada_day = datetime(2018,7 ,3)\n",
    "\n",
    "evo_data.loc[((evo_data.index > canada_day) & (evo_data.index <= end_canada_day))][\"holidays\"] = 1\n",
    "modo_data.loc[((modo_data.index > canada_day) & (modo_data.index <= end_canada_day))][\"holidays\"] = 1\n",
    "c2g_data.loc[((c2g_data.index > canada_day) & (c2g_data.index <= end_canada_day))][\"holidays\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_period = '06-23'\n",
    "end_period = '07-12'\n",
    "\n",
    "evo_data = evo_data[(evo_data.index >= '2018-'+init_period) & (evo_data.index <= '2018-'+end_period)]\n",
    "modo_data = modo_data[(modo_data.index >= '2018-'+init_period) & (modo_data.index <= '2018-'+end_period)]\n",
    "c2g_data = c2g_data.loc[\"2016-12-13 15:00:00\":\"2017-02-25 17:00:00\"]\n",
    "\n",
    "unievo_data = unievo_data[(unievo_data.index >= '2018-'+init_period) & (unievo_data.index <= '2018-'+end_period)]\n",
    "unimodo_data = unimodo_data[(unimodo_data.index >= '2018-'+init_period) & (unimodo_data.index <= '2018-'+end_period)]\n",
    "unic2g_data = unic2g_data.loc[\"2016-12-13 15:00:00\":\"2017-02-25 17:00:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sup_learning_formatter(data, past_lags, future_steps, train_split):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    norm_data  = data.values\n",
    "\n",
    "    for n in range(len(data) - past_lags - future_steps):\n",
    "        X.append(norm_data[n : n + past_lags])\n",
    "        y.append(data.travels.values[n + past_lags : n + past_lags + future_steps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_splitter(data, splits):\n",
    "    locs = [int(len(data)*n) for n in splits]\n",
    "    return data[:locs[0]], data[locs[0]:locs[1]], data[locs[1]:], data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(y, y_hat):\n",
    "    evaluation = {}\n",
    "    evaluation[\"RMSE\"] = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    evaluation[\"MAE\"] = mean_absolute_error(y, y_hat)\n",
    "    evaluation[\"R2\"] = r2_score(y, y_hat)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistance_model(X, timesteps):\n",
    "    y_hat = []\n",
    "    for x in X:\n",
    "        y_hat.append(np.array([x[-1][0] for _ in range(timesteps)]))\n",
    "\n",
    "    return np.array(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history, title, save_file=False):\n",
    "    history = pd.DataFrame(history.history)\n",
    "\n",
    "    history.plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"plots\\\\\" + title.replace(\" \", \"_\") + \".png\", bbox_inches='tight') if save_file else print()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, will be generated the model for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearchLSTM:\n",
    "    def __init__(self):\n",
    "        self.evaluations = pd.DataFrame()\n",
    "        self.best_estimator = None\n",
    "\n",
    "    def search(self, feature_dict, data, verbose=1, windows=1, splits = (0.6, 0.8)):\n",
    "        \n",
    "        def average_evaluations(validation_eval, key=\"val_loss\"):\n",
    "            acc_value = 0\n",
    "            for split, evaluation in validation_eval:\n",
    "                acc_value += evaluation[key]\n",
    "            return acc_value/len(validation_eval)\n",
    "        \n",
    "        possibilities_list = self._create_feature_dict(feature_dict)\n",
    "        current_evaluations = []\n",
    "        if(windows == 1):\n",
    "            for test in tqdm(possibilities_list):\n",
    "                model, hist, test_data, evaluation = self.run_lstm(data, 24, 12, splits, verbose=verbose, **test)\n",
    "                validation_eval = {key:value[-1] for key, value in hist.history.items()}\n",
    "                current_evaluations.append([test, validation_eval])\n",
    "        \n",
    "        else:\n",
    "            increase = splits[1]/(windows + 1)\n",
    "            for test in tqdm(possibilities_list):\n",
    "                validation_eval = []\n",
    "                for i in tqdm(range(windows)):\n",
    "                    cur_split = (increase*(i + 1), increase*(i + 2))\n",
    "                    model, hist, test_data, evaluation = self.run_lstm(data, 24, 12, cur_split, verbose=verbose, **test)\n",
    "                    cur_validation_eval = {key:value[-1] for key, value in hist.history.items()}\n",
    "                    validation_eval.append([cur_split, cur_validation_eval])\n",
    "                    print(cur_split)\n",
    "                current_evaluations.append([test, validation_eval])\n",
    "        \n",
    "        if(windows == 1):\n",
    "            current_evaluations.sort(key=lambda x: x[1][\"val_loss\"])\n",
    "        else:\n",
    "            current_evaluations.sort(key=lambda x: average_evaluations(x[1]))   \n",
    "        \n",
    "        self.evaluations = pd.DataFrame(map(lambda x: {**x[0], **x[1]}, current_evaluations))\n",
    "        self.best_estimator = current_evaluations[0][0]\n",
    "            \n",
    "\n",
    "    def _create_feature_dict(self, feature_dict):\n",
    "        return self._create_feature_dict_recurse({}, feature_dict, list(feature_dict.keys()))\n",
    "\n",
    "    def _create_feature_dict_recurse(self, start_dict, feature_dict, remaining_keys):\n",
    "        if len(remaining_keys) == 0:\n",
    "            return [start_dict]\n",
    "        new_feature_dict = feature_dict.copy()\n",
    "        returned_list = []\n",
    "        del new_feature_dict[remaining_keys[0]]\n",
    "        for item in feature_dict[remaining_keys[0]]:\n",
    "            new_start_dict = start_dict.copy()\n",
    "            new_start_dict[remaining_keys[0]] = item\n",
    "            returned_list += self._create_feature_dict_recurse(new_start_dict, new_feature_dict, remaining_keys[1:])\n",
    "        return returned_list\n",
    "\n",
    "\n",
    "    def run_lstm(self, data, past_lags, future_steps, splits, node_number=50,\n",
    "                 epochs=10, batch_size=64, loss='mae', dropout=0.5, layer_count=2, verbose=1):\n",
    "        \n",
    "        X, y = sup_learning_formatter(data, past_lags, future_steps, splits[0])\n",
    "        X_train, X_val, X_test, X_shape = train_val_test_splitter(X, splits)\n",
    "        y_train, y_val, y_test, y_shape = train_val_test_splitter(y, splits)\n",
    "\n",
    "\n",
    "        train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train = train.cache().shuffle(batch_size).batch(batch_size).repeat()\n",
    "\n",
    "        val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        val = val.batch(batch_size).repeat()\n",
    "\n",
    "        model = tf.keras.models.Sequential()\n",
    "\n",
    "        if(layer_count == 1):\n",
    "            model.add(tf.keras.layers.LSTM(node_number,\n",
    "                                    input_shape=X_shape))\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.LSTM(node_number, return_sequences=True,\n",
    "                                    input_shape=X_shape))\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "            for _ in range(layer_count - 2):\n",
    "                model.add(tf.keras.layers.LSTM(node_number, return_sequences=True, activation='relu'))\n",
    "\n",
    "            model.add(tf.keras.layers.LSTM(node_number, activation='relu'))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(12))\n",
    "        \n",
    "        def rmse(y_true, y_pred):\n",
    "            return tf.sqrt(tf.reduce_mean((y_true - y_pred)**2))\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss=loss, metrics=[rmse])\n",
    "        \n",
    "        history = model.fit(train, epochs=epochs, steps_per_epoch=50,\n",
    "                            validation_data=val, validation_steps=50, verbose=verbose\n",
    "                            )\n",
    "        y_hat_test = model.predict(X_test)\n",
    "        evaluation = eval_model(y_test, y_hat_test)\n",
    "\n",
    "        return model, history, (X_test, y_test), evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d768821fd94154a588fcf73f7719d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 126.5316 - rmse: 158.0425 - val_loss: 68.7451 - val_rmse: 84.4685\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 73.1685 - rmse: 90.5638 - val_loss: 72.9477 - val_rmse: 90.8810\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 57.6472 - rmse: 72.7321 - val_loss: 45.1816 - val_rmse: 56.6637\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 49.5288 - rmse: 63.8220 - val_loss: 40.0666 - val_rmse: 52.0412\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 44.5447 - rmse: 58.1149 - val_loss: 40.4173 - val_rmse: 53.2434\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 40.8660 - rmse: 53.7451 - val_loss: 44.9528 - val_rmse: 57.5953\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 39.9096 - rmse: 52.4519 - val_loss: 37.9269 - val_rmse: 50.4574\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 10s 206ms/step - loss: 39.1540 - rmse: 51.4776 - val_loss: 34.3181 - val_rmse: 45.6019\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 36.4961 - rmse: 48.5505 - val_loss: 34.7226 - val_rmse: 45.8183\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 36.1538 - rmse: 48.1389 - val_loss: 34.3504 - val_rmse: 45.9444\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 34.9698 - rmse: 46.8349 - val_loss: 33.7636 - val_rmse: 45.3809\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 33.5678 - rmse: 45.0507 - val_loss: 31.5687 - val_rmse: 42.4215\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 32.9911 - rmse: 44.1182 - val_loss: 36.4841 - val_rmse: 47.9423\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 32.0607 - rmse: 42.9036 - val_loss: 30.8242 - val_rmse: 40.7511\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 32.2009 - rmse: 42.8917 - val_loss: 33.2274 - val_rmse: 43.6463\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 31.1509 - rmse: 41.6396 - val_loss: 47.0176 - val_rmse: 60.6893\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 30.9824 - rmse: 41.3597 - val_loss: 33.5893 - val_rmse: 43.8183\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 29.6203 - rmse: 39.7179 - val_loss: 34.3298 - val_rmse: 45.6338\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 29.7951 - rmse: 39.9997 - val_loss: 41.8193 - val_rmse: 54.6359\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 28.5297 - rmse: 38.2692 - val_loss: 32.3834 - val_rmse: 42.2777\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 28.5191 - rmse: 38.5445 - val_loss: 35.3302 - val_rmse: 46.9797\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 28.5632 - rmse: 38.5649 - val_loss: 29.1668 - val_rmse: 38.7554\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 27.4053 - rmse: 36.9487 - val_loss: 36.8067 - val_rmse: 48.4988\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 27.3664 - rmse: 36.9773 - val_loss: 28.6596 - val_rmse: 38.3731\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 26.6513 - rmse: 35.9712 - val_loss: 27.7725 - val_rmse: 37.6564\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 10s 204ms/step - loss: 130.1031 - rmse: 160.2337 - val_loss: 66.5240 - val_rmse: 81.1105\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 6s 114ms/step - loss: 73.1304 - rmse: 89.9460 - val_loss: 62.1546 - val_rmse: 75.1742\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 68.7291 - rmse: 84.8971 - val_loss: 56.6831 - val_rmse: 71.2467\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 58.8816 - rmse: 73.9060 - val_loss: 66.5028 - val_rmse: 81.0786\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 50.8607 - rmse: 65.1717 - val_loss: 48.2557 - val_rmse: 64.1700\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 45.6217 - rmse: 59.4502 - val_loss: 59.7079 - val_rmse: 71.6348\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 44.8898 - rmse: 58.6112 - val_loss: 44.2604 - val_rmse: 58.3744\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 42.0840 - rmse: 55.6835 - val_loss: 35.3555 - val_rmse: 47.1913\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 40.6896 - rmse: 53.9937 - val_loss: 36.3408 - val_rmse: 48.6288\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 39.1972 - rmse: 51.7674 - val_loss: 42.7822 - val_rmse: 56.5975\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 37.6700 - rmse: 50.0229 - val_loss: 35.1276 - val_rmse: 46.8460\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 37.5398 - rmse: 49.6394 - val_loss: 34.2421 - val_rmse: 45.3155\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 35.9097 - rmse: 47.7820 - val_loss: 45.3206 - val_rmse: 58.3824\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 34.2878 - rmse: 45.4324 - val_loss: 33.2986 - val_rmse: 43.3730\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 34.5817 - rmse: 45.9293 - val_loss: 38.4813 - val_rmse: 50.6754\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 33.1330 - rmse: 44.0925 - val_loss: 40.2199 - val_rmse: 52.1992\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 32.7960 - rmse: 43.6829 - val_loss: 51.2908 - val_rmse: 65.2490\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 32.0268 - rmse: 42.5839 - val_loss: 46.1981 - val_rmse: 60.1240\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 31.7083 - rmse: 42.2275 - val_loss: 31.2135 - val_rmse: 41.3231\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 30.8669 - rmse: 41.2566 - val_loss: 30.7368 - val_rmse: 40.8391\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 30.1832 - rmse: 40.4389 - val_loss: 33.9364 - val_rmse: 44.1798\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 29.7746 - rmse: 40.0609 - val_loss: 29.3541 - val_rmse: 39.3593\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 29.6170 - rmse: 39.8099 - val_loss: 31.6866 - val_rmse: 42.1486\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 29.9467 - rmse: 39.9132 - val_loss: 28.8454 - val_rmse: 38.3337\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 28.2412 - rmse: 37.9368 - val_loss: 31.4743 - val_rmse: 42.2391\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 105.2041 - rmse: 129.9952 - val_loss: 64.5734 - val_rmse: 79.0629\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 68.1045 - rmse: 84.0960 - val_loss: 81.0368 - val_rmse: 99.9023\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 56.9111 - rmse: 72.3185 - val_loss: 43.6644 - val_rmse: 55.2386\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 49.7793 - rmse: 64.3739 - val_loss: 44.8117 - val_rmse: 56.8112\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 5s 105ms/step - loss: 44.1048 - rmse: 57.4056 - val_loss: 38.2878 - val_rmse: 50.4820\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 41.5299 - rmse: 54.5275 - val_loss: 42.1640 - val_rmse: 53.0723\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 39.7307 - rmse: 52.2606 - val_loss: 40.0250 - val_rmse: 52.5355\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 5s 106ms/step - loss: 38.1922 - rmse: 50.5897 - val_loss: 39.4187 - val_rmse: 51.1006\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 36.9489 - rmse: 48.8875 - val_loss: 35.3305 - val_rmse: 46.6493\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 36.3607 - rmse: 47.9175 - val_loss: 36.9840 - val_rmse: 49.3554\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 34.8188 - rmse: 46.3842 - val_loss: 35.0738 - val_rmse: 46.7986\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 34.0312 - rmse: 45.0502 - val_loss: 32.2339 - val_rmse: 43.2655\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 33.1363 - rmse: 44.1568 - val_loss: 35.8717 - val_rmse: 47.4520\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 5s 105ms/step - loss: 31.8621 - rmse: 42.4388 - val_loss: 39.6345 - val_rmse: 52.2185\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 32.3704 - rmse: 42.7980 - val_loss: 40.0973 - val_rmse: 52.0104\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 31.6502 - rmse: 41.9445 - val_loss: 33.7833 - val_rmse: 43.7086\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 30.3756 - rmse: 40.5675 - val_loss: 45.2815 - val_rmse: 58.9310\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 30.0540 - rmse: 40.1336 - val_loss: 36.0235 - val_rmse: 47.0279\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 29.7264 - rmse: 39.7275 - val_loss: 32.1687 - val_rmse: 42.2818\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 29.0401 - rmse: 38.8534 - val_loss: 43.5126 - val_rmse: 55.3878\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 29.4482 - rmse: 39.2604 - val_loss: 32.2175 - val_rmse: 42.9514\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 28.5140 - rmse: 38.1448 - val_loss: 28.5768 - val_rmse: 37.8378\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 31.8326 - rmse: 83.5224 - val_loss: 31.0445 - val_rmse: 41.3392\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 26.9922 - rmse: 36.2946 - val_loss: 29.6020 - val_rmse: 39.0490\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 27.1517 - rmse: 36.6238 - val_loss: 28.9340 - val_rmse: 38.5838\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 10s 209ms/step - loss: 98.3302 - rmse: 120.2110 - val_loss: 100.3996 - val_rmse: 121.7764\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 73.7007 - rmse: 91.6493 - val_loss: 68.4196 - val_rmse: 85.3572\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 64.9638 - rmse: 81.6117 - val_loss: 46.7201 - val_rmse: 60.4017\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 5s 105ms/step - loss: 51.3197 - rmse: 65.9371 - val_loss: 65.0388 - val_rmse: 82.6041\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 47.3693 - rmse: 61.6222 - val_loss: 48.9053 - val_rmse: 64.8535\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 43.7948 - rmse: 57.1377 - val_loss: 40.0304 - val_rmse: 53.8476\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 110ms/step - loss: 41.9652 - rmse: 55.0377 - val_loss: 37.6737 - val_rmse: 50.2097\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 40.3317 - rmse: 53.1734 - val_loss: 37.7949 - val_rmse: 50.1708\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 38.4643 - rmse: 50.8312 - val_loss: 36.1737 - val_rmse: 48.5925\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 37.4213 - rmse: 49.5814 - val_loss: 40.8749 - val_rmse: 54.8721\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 36.7743 - rmse: 48.7672 - val_loss: 39.0284 - val_rmse: 52.2269\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 34.9128 - rmse: 46.5161 - val_loss: 33.6497 - val_rmse: 44.7069\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 34.4568 - rmse: 46.1732 - val_loss: 38.2607 - val_rmse: 50.7371\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 34.2776 - rmse: 45.5587 - val_loss: 32.7971 - val_rmse: 43.6678\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 33.2519 - rmse: 44.2286 - val_loss: 39.3019 - val_rmse: 51.5796\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 108ms/step - loss: 33.0489 - rmse: 44.0297 - val_loss: 38.3921 - val_rmse: 48.4039\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 32.7969 - rmse: 43.4884 - val_loss: 35.3185 - val_rmse: 46.0086\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 31.6853 - rmse: 42.3206 - val_loss: 33.8759 - val_rmse: 44.6116\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 108ms/step - loss: 31.1320 - rmse: 41.5579 - val_loss: 41.2532 - val_rmse: 53.3868\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 30.7707 - rmse: 41.2926 - val_loss: 31.3009 - val_rmse: 41.6725\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 30.1454 - rmse: 40.4005 - val_loss: 32.8947 - val_rmse: 43.1695\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 6s 112ms/step - loss: 29.9155 - rmse: 40.2183 - val_loss: 32.2194 - val_rmse: 42.3395\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 29.9553 - rmse: 40.2648 - val_loss: 33.4063 - val_rmse: 43.6006\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 29.0137 - rmse: 39.2593 - val_loss: 29.7946 - val_rmse: 39.5719\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 28.6284 - rmse: 38.5940 - val_loss: 29.4333 - val_rmse: 39.1677\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 10s 203ms/step - loss: 109.0875 - rmse: 131.1799 - val_loss: 64.9319 - val_rmse: 80.4487\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 5s 105ms/step - loss: 69.6941 - rmse: 86.9803 - val_loss: 62.1588 - val_rmse: 79.3174\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 57.8798 - rmse: 73.5396 - val_loss: 48.8724 - val_rmse: 62.3032\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 50.2357 - rmse: 65.2041 - val_loss: 40.4611 - val_rmse: 51.6788\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 5s 108ms/step - loss: 46.6045 - rmse: 60.8956 - val_loss: 54.1737 - val_rmse: 71.1822\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 44.1914 - rmse: 58.2182 - val_loss: 38.7977 - val_rmse: 51.7213\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 41.7894 - rmse: 55.3348 - val_loss: 39.5725 - val_rmse: 52.6906\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 40.1481 - rmse: 52.9987 - val_loss: 38.5216 - val_rmse: 51.7069\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 38.9678 - rmse: 51.7209 - val_loss: 37.6499 - val_rmse: 50.6245\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 38.1177 - rmse: 50.7186 - val_loss: 42.1669 - val_rmse: 56.0310\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 5s 105ms/step - loss: 37.5219 - rmse: 49.8235 - val_loss: 47.8057 - val_rmse: 62.5946\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 35.8612 - rmse: 47.7987 - val_loss: 38.7244 - val_rmse: 51.3698\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 35.7648 - rmse: 47.5092 - val_loss: 48.2075 - val_rmse: 62.5253\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 33.6753 - rmse: 44.7500 - val_loss: 33.7266 - val_rmse: 44.9664\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 33.7081 - rmse: 45.0057 - val_loss: 53.3322 - val_rmse: 67.8597\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 32.9494 - rmse: 44.1899 - val_loss: 41.8289 - val_rmse: 54.5328\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 33.3401 - rmse: 44.5646 - val_loss: 39.6332 - val_rmse: 51.1257\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 6s 125ms/step - loss: 32.7105 - rmse: 43.7023 - val_loss: 32.6809 - val_rmse: 42.9964\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 32.1946 - rmse: 43.1315 - val_loss: 33.6309 - val_rmse: 44.2619\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 31.9231 - rmse: 42.6517 - val_loss: 31.7806 - val_rmse: 41.7416\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 31.2527 - rmse: 42.0112 - val_loss: 32.2872 - val_rmse: 41.7920\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 30.5873 - rmse: 41.0591 - val_loss: 39.9851 - val_rmse: 51.8884\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 30.4255 - rmse: 41.2341 - val_loss: 31.0991 - val_rmse: 40.7399\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 6s 114ms/step - loss: 31.1031 - rmse: 41.5495 - val_loss: 43.3822 - val_rmse: 56.0639\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 30.3492 - rmse: 40.7290 - val_loss: 32.4689 - val_rmse: 43.7986\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 634.3812 - rmse: 787.9316 - val_loss: 77.2008 - val_rmse: 93.6272\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 149ms/step - loss: 73.4453 - rmse: 90.1048 - val_loss: 66.9761 - val_rmse: 82.3327\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 66.2804 - rmse: 82.3641 - val_loss: 51.8513 - val_rmse: 64.4320\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 55.8212 - rmse: 70.9392 - val_loss: 49.0899 - val_rmse: 63.1241\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 48.9096 - rmse: 63.1478 - val_loss: 43.9426 - val_rmse: 56.7539\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 44.7887 - rmse: 58.5941 - val_loss: 45.3419 - val_rmse: 57.3728\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 41.5168 - rmse: 54.6331 - val_loss: 37.1802 - val_rmse: 48.5035\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 40.0428 - rmse: 52.7524 - val_loss: 40.8262 - val_rmse: 53.1521\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 38.0956 - rmse: 50.2507 - val_loss: 37.1026 - val_rmse: 49.4967\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 37.8875 - rmse: 50.4168 - val_loss: 35.0336 - val_rmse: 46.6544\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 36.5628 - rmse: 48.4682 - val_loss: 36.6389 - val_rmse: 48.8027\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 35.7388 - rmse: 47.3001 - val_loss: 34.0620 - val_rmse: 45.7896\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 34.3572 - rmse: 45.8871 - val_loss: 39.0705 - val_rmse: 51.1084\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 33.3898 - rmse: 44.3502 - val_loss: 38.6861 - val_rmse: 49.9616\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 33.0516 - rmse: 44.0330 - val_loss: 30.7421 - val_rmse: 40.8422\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 32.2017 - rmse: 42.8874 - val_loss: 36.2568 - val_rmse: 47.8576\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 7s 147ms/step - loss: 30.6770 - rmse: 41.2947 - val_loss: 37.8969 - val_rmse: 49.5308\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 30.4209 - rmse: 40.8024 - val_loss: 41.5546 - val_rmse: 54.2072\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 29.5497 - rmse: 39.7085 - val_loss: 31.5073 - val_rmse: 41.2731\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 29.4454 - rmse: 39.3631 - val_loss: 42.3074 - val_rmse: 55.8003\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 28.9752 - rmse: 39.0907 - val_loss: 38.5531 - val_rmse: 49.8169\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 28.3431 - rmse: 38.0443 - val_loss: 42.4634 - val_rmse: 54.6862\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 28.1771 - rmse: 37.7908 - val_loss: 33.7068 - val_rmse: 43.9970\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 27.2283 - rmse: 36.5927 - val_loss: 28.5548 - val_rmse: 38.3293\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 26.8856 - rmse: 36.3937 - val_loss: 30.9164 - val_rmse: 40.9325\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 263.2878 - rmse: 326.5670 - val_loss: 90.2607 - val_rmse: 110.8613\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 77.0397 - rmse: 94.3567 - val_loss: 63.2022 - val_rmse: 78.1623\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 66.6244 - rmse: 82.4713 - val_loss: 56.0163 - val_rmse: 67.7627\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 61.0169 - rmse: 77.3441 - val_loss: 57.3820 - val_rmse: 73.9649\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 49.0716 - rmse: 63.0748 - val_loss: 65.1778 - val_rmse: 80.1852\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 48.6446 - rmse: 62.5844 - val_loss: 42.3373 - val_rmse: 52.9141\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 43.0297 - rmse: 55.9290 - val_loss: 37.5729 - val_rmse: 50.1328\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 6s 123ms/step - loss: 41.9579 - rmse: 54.7351 - val_loss: 39.4825 - val_rmse: 52.4060\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 38.7753 - rmse: 51.0457 - val_loss: 36.6789 - val_rmse: 48.7304\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 38.0942 - rmse: 50.1614 - val_loss: 45.9421 - val_rmse: 60.1918\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 36.5485 - rmse: 48.3531 - val_loss: 34.5949 - val_rmse: 46.1196\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 142ms/step - loss: 34.9385 - rmse: 46.3405 - val_loss: 34.2177 - val_rmse: 45.1119\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 35.0130 - rmse: 46.3787 - val_loss: 39.4854 - val_rmse: 51.5910\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 34.0142 - rmse: 45.1652 - val_loss: 34.3111 - val_rmse: 44.7184\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 124ms/step - loss: 32.8696 - rmse: 43.9106 - val_loss: 30.5863 - val_rmse: 40.5718\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 31.2431 - rmse: 41.6159 - val_loss: 37.2265 - val_rmse: 48.7296\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 6s 121ms/step - loss: 31.0670 - rmse: 41.5605 - val_loss: 47.1557 - val_rmse: 60.3126\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 6s 121ms/step - loss: 30.2650 - rmse: 40.6499 - val_loss: 31.0115 - val_rmse: 40.4869\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 30.0336 - rmse: 40.1321 - val_loss: 32.9435 - val_rmse: 42.5337\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 29.5826 - rmse: 39.8223 - val_loss: 40.5675 - val_rmse: 52.3857\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 29.6669 - rmse: 39.6724 - val_loss: 30.9658 - val_rmse: 41.0585\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 28.2069 - rmse: 38.0178 - val_loss: 29.4462 - val_rmse: 38.8370\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 28.2466 - rmse: 38.1168 - val_loss: 32.9646 - val_rmse: 42.9269\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 26.7205 - rmse: 36.1953 - val_loss: 30.6524 - val_rmse: 40.5741\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 27.3121 - rmse: 36.9868 - val_loss: 28.4173 - val_rmse: 37.8787\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 189.1534 - rmse: 232.9976 - val_loss: 70.2890 - val_rmse: 86.6205\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 72.4209 - rmse: 89.6646 - val_loss: 55.1186 - val_rmse: 67.3793\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 61.1015 - rmse: 77.2378 - val_loss: 51.6779 - val_rmse: 65.8218\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 53.4268 - rmse: 69.0046 - val_loss: 40.1567 - val_rmse: 52.7445\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 46.8707 - rmse: 60.8407 - val_loss: 47.5629 - val_rmse: 60.6709\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 44.3007 - rmse: 58.0540 - val_loss: 52.2916 - val_rmse: 66.2591\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 42.2168 - rmse: 56.0600 - val_loss: 38.1552 - val_rmse: 50.3726\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 40.4543 - rmse: 53.7031 - val_loss: 35.5742 - val_rmse: 47.3061\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 38.8756 - rmse: 51.7748 - val_loss: 34.4706 - val_rmse: 46.3911\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 37.6640 - rmse: 50.3080 - val_loss: 41.0492 - val_rmse: 55.0689\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 36.8817 - rmse: 49.3115 - val_loss: 35.5777 - val_rmse: 48.0685\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 36.0835 - rmse: 48.4954 - val_loss: 32.8442 - val_rmse: 44.0842\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 34.7583 - rmse: 46.4741 - val_loss: 40.1184 - val_rmse: 53.1718\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 33.7274 - rmse: 45.3240 - val_loss: 33.5762 - val_rmse: 44.9654\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 33.4779 - rmse: 44.9119 - val_loss: 35.8353 - val_rmse: 46.4682\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 32.4076 - rmse: 43.5469 - val_loss: 32.7529 - val_rmse: 43.0747\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 32.7366 - rmse: 43.8936 - val_loss: 50.3005 - val_rmse: 64.1327\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 31.6719 - rmse: 42.4316 - val_loss: 48.1710 - val_rmse: 61.1026\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 31.7711 - rmse: 42.5339 - val_loss: 38.4096 - val_rmse: 49.8552\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 30.6968 - rmse: 41.2196 - val_loss: 30.4204 - val_rmse: 40.6112\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 29.6400 - rmse: 40.1130 - val_loss: 38.1941 - val_rmse: 49.1626\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 30.4450 - rmse: 40.8800 - val_loss: 33.0378 - val_rmse: 43.4322\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 28.8153 - rmse: 38.8150 - val_loss: 37.6652 - val_rmse: 49.4230\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 28.8040 - rmse: 38.7651 - val_loss: 36.9934 - val_rmse: 48.0853\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 28.1475 - rmse: 37.9494 - val_loss: 33.6037 - val_rmse: 44.4567\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 235.3470 - rmse: 281.6567 - val_loss: 67.6414 - val_rmse: 82.6852\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 77.7907 - rmse: 95.7602 - val_loss: 86.9927 - val_rmse: 106.4568\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 72.0086 - rmse: 89.5956 - val_loss: 55.8615 - val_rmse: 68.4077\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 59.6638 - rmse: 75.6815 - val_loss: 47.1008 - val_rmse: 60.2743\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 50.0982 - rmse: 64.5065 - val_loss: 70.5389 - val_rmse: 90.1645\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 46.2697 - rmse: 60.3133 - val_loss: 41.7024 - val_rmse: 53.0583\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 42.9349 - rmse: 56.1200 - val_loss: 39.5337 - val_rmse: 52.7940\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 40.9389 - rmse: 53.8015 - val_loss: 38.2686 - val_rmse: 51.0507\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 39.6188 - rmse: 52.4255 - val_loss: 35.2938 - val_rmse: 47.4887\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 37.6643 - rmse: 50.0498 - val_loss: 41.6320 - val_rmse: 55.1682\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 35.8307 - rmse: 47.8324 - val_loss: 40.7774 - val_rmse: 54.0396\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 34.7484 - rmse: 46.6993 - val_loss: 34.5844 - val_rmse: 46.0461\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 34.7227 - rmse: 46.1754 - val_loss: 43.7845 - val_rmse: 56.9752\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 33.2974 - rmse: 44.2315 - val_loss: 34.1619 - val_rmse: 44.5547\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 31.9493 - rmse: 42.7853 - val_loss: 32.2310 - val_rmse: 42.4408\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 32.1856 - rmse: 42.7804 - val_loss: 31.8455 - val_rmse: 41.9102\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 31.2842 - rmse: 41.8232 - val_loss: 33.8318 - val_rmse: 43.8695\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 31.1026 - rmse: 41.6177 - val_loss: 37.1906 - val_rmse: 48.8028\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 30.1402 - rmse: 40.3637 - val_loss: 45.2327 - val_rmse: 57.4723\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 30.2751 - rmse: 40.4231 - val_loss: 31.1876 - val_rmse: 41.2662\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 30.8115 - rmse: 41.5910 - val_loss: 30.7931 - val_rmse: 40.6386\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 28.9859 - rmse: 39.0363 - val_loss: 33.9191 - val_rmse: 44.4119\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 29.0252 - rmse: 38.8923 - val_loss: 30.9359 - val_rmse: 40.8778\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 28.6069 - rmse: 38.4894 - val_loss: 33.2044 - val_rmse: 43.9220\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 28.1430 - rmse: 38.0613 - val_loss: 29.2661 - val_rmse: 39.0575\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 104.2945 - rmse: 128.1428 - val_loss: 87.9224 - val_rmse: 107.6128\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 145ms/step - loss: 71.4560 - rmse: 87.8186 - val_loss: 66.1612 - val_rmse: 82.7051\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 63.9021 - rmse: 79.2222 - val_loss: 55.1476 - val_rmse: 70.9427\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 52.3446 - rmse: 66.6334 - val_loss: 42.6950 - val_rmse: 54.6693\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 45.0614 - rmse: 58.6652 - val_loss: 39.1099 - val_rmse: 51.5543\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 42.9650 - rmse: 56.2095 - val_loss: 37.7192 - val_rmse: 49.9151\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 41.3260 - rmse: 54.0705 - val_loss: 46.4731 - val_rmse: 61.2930\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 39.0507 - rmse: 51.4081 - val_loss: 36.6641 - val_rmse: 49.2993\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 38.3443 - rmse: 50.6481 - val_loss: 36.5003 - val_rmse: 49.3820\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 37.1449 - rmse: 49.1038 - val_loss: 35.1830 - val_rmse: 47.2865\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 36.3310 - rmse: 47.7900 - val_loss: 39.6422 - val_rmse: 52.7246\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 35.9207 - rmse: 47.6632 - val_loss: 35.4459 - val_rmse: 47.5906\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 34.7216 - rmse: 45.9360 - val_loss: 42.8212 - val_rmse: 56.2836\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 34.4010 - rmse: 45.4794 - val_loss: 32.6298 - val_rmse: 43.6730\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 32.6246 - rmse: 43.3690 - val_loss: 32.3183 - val_rmse: 42.9983\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 145ms/step - loss: 32.8704 - rmse: 43.6669 - val_loss: 34.8912 - val_rmse: 45.8565\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 32.1654 - rmse: 42.8767 - val_loss: 33.3571 - val_rmse: 43.4504\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 31.4030 - rmse: 41.9806 - val_loss: 33.5688 - val_rmse: 44.5980\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 30.8483 - rmse: 41.4341 - val_loss: 34.8540 - val_rmse: 44.8458\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 30.2514 - rmse: 40.5748 - val_loss: 30.1568 - val_rmse: 39.9824\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 30.0977 - rmse: 40.4272 - val_loss: 31.2989 - val_rmse: 40.8255\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 29.3039 - rmse: 39.2197 - val_loss: 29.6376 - val_rmse: 39.1353\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 29.0962 - rmse: 38.7349 - val_loss: 29.0589 - val_rmse: 38.3297\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 29.3445 - rmse: 39.2388 - val_loss: 32.0101 - val_rmse: 42.1897\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 28.5035 - rmse: 38.1267 - val_loss: 30.1088 - val_rmse: 40.4907\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 15s 292ms/step - loss: 168.2848 - rmse: 198.8997 - val_loss: 58.6139 - val_rmse: 72.0547\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 8s 162ms/step - loss: 68.4182 - rmse: 84.3253 - val_loss: 80.7675 - val_rmse: 99.9882\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 150ms/step - loss: 68.3115 - rmse: 85.1296 - val_loss: 72.8340 - val_rmse: 92.2938\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 57.5319 - rmse: 73.4446 - val_loss: 66.9298 - val_rmse: 80.4668\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 47.4692 - rmse: 61.7623 - val_loss: 47.1515 - val_rmse: 63.1570\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 45.8019 - rmse: 59.7262 - val_loss: 49.2475 - val_rmse: 61.4869\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 8s 162ms/step - loss: 41.5202 - rmse: 54.5796 - val_loss: 46.8778 - val_rmse: 61.6918\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 39.2541 - rmse: 51.7905 - val_loss: 38.7532 - val_rmse: 49.6335\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 37.6342 - rmse: 49.8702 - val_loss: 34.2449 - val_rmse: 45.7860\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 35.7893 - rmse: 47.2915 - val_loss: 46.2377 - val_rmse: 60.8236\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 35.4408 - rmse: 46.8412 - val_loss: 34.9750 - val_rmse: 46.6040\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 32.4184 - rmse: 43.1836 - val_loss: 35.1324 - val_rmse: 46.9239\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 8s 157ms/step - loss: 31.9868 - rmse: 42.5672 - val_loss: 31.9531 - val_rmse: 42.6910\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 31.6476 - rmse: 42.1174 - val_loss: 36.2745 - val_rmse: 47.3210\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 30.7416 - rmse: 40.8447 - val_loss: 30.3077 - val_rmse: 40.3816\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 29.9112 - rmse: 39.7838 - val_loss: 32.0204 - val_rmse: 42.3497\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 28.5976 - rmse: 38.2579 - val_loss: 47.3820 - val_rmse: 60.8588\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 27.8774 - rmse: 37.5697 - val_loss: 31.4738 - val_rmse: 41.6825\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 28.1074 - rmse: 37.7041 - val_loss: 31.6806 - val_rmse: 42.3788\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 27.2725 - rmse: 36.6073 - val_loss: 28.5471 - val_rmse: 38.0817\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 27.0126 - rmse: 36.4684 - val_loss: 32.8187 - val_rmse: 43.7015\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 26.3466 - rmse: 35.7057 - val_loss: 27.7354 - val_rmse: 36.9611\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 26.1226 - rmse: 35.2866 - val_loss: 36.0229 - val_rmse: 46.6785\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 25.7030 - rmse: 34.7620 - val_loss: 28.9609 - val_rmse: 38.5110\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 25.8553 - rmse: 35.1740 - val_loss: 30.4511 - val_rmse: 40.4033\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 13s 268ms/step - loss: 88.8141 - rmse: 108.2799 - val_loss: 63.2559 - val_rmse: 75.3561\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 8s 161ms/step - loss: 59.9545 - rmse: 75.3067 - val_loss: 45.1356 - val_rmse: 58.4456\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 49.5165 - rmse: 64.1612 - val_loss: 39.3179 - val_rmse: 52.2214\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 8s 162ms/step - loss: 44.8011 - rmse: 58.6532 - val_loss: 43.9328 - val_rmse: 54.6636\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 40.6924 - rmse: 53.6821 - val_loss: 38.6110 - val_rmse: 51.5533\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 38.7466 - rmse: 51.5344 - val_loss: 34.8918 - val_rmse: 45.3482\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 38.0040 - rmse: 50.3574 - val_loss: 36.6118 - val_rmse: 47.2084\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 36.6581 - rmse: 48.7538 - val_loss: 34.9721 - val_rmse: 46.6592\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 35.7163 - rmse: 47.6905 - val_loss: 34.0802 - val_rmse: 44.7607\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 34.1368 - rmse: 45.7175 - val_loss: 32.7578 - val_rmse: 43.7425\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 33.5629 - rmse: 44.9215 - val_loss: 38.8848 - val_rmse: 50.9489\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 32.5074 - rmse: 43.6610 - val_loss: 32.0542 - val_rmse: 42.7671\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 31.7194 - rmse: 42.5584 - val_loss: 40.4550 - val_rmse: 53.0295\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 31.6306 - rmse: 42.0278 - val_loss: 37.6187 - val_rmse: 48.7939\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 8s 161ms/step - loss: 30.8447 - rmse: 41.2582 - val_loss: 29.9916 - val_rmse: 39.2758\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 29.6548 - rmse: 40.0391 - val_loss: 30.9262 - val_rmse: 40.6461\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 29.5098 - rmse: 39.7477 - val_loss: 35.2197 - val_rmse: 45.6413\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 29.8348 - rmse: 40.0196 - val_loss: 42.1024 - val_rmse: 55.6494\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 28.9477 - rmse: 38.7783 - val_loss: 34.7777 - val_rmse: 46.4380\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 28.3719 - rmse: 38.2552 - val_loss: 36.6128 - val_rmse: 48.4951\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 27.3581 - rmse: 37.0544 - val_loss: 29.9702 - val_rmse: 38.9473\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 27.3920 - rmse: 37.2000 - val_loss: 28.3497 - val_rmse: 38.0488\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 27.7028 - rmse: 37.4565 - val_loss: 34.5621 - val_rmse: 45.5181\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 26.4777 - rmse: 35.5834 - val_loss: 26.6729 - val_rmse: 35.7291\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 26.4034 - rmse: 35.7447 - val_loss: 29.2211 - val_rmse: 39.4930\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 763.6199 - rmse: 1005.0748 - val_loss: 120.1760 - val_rmse: 147.8849\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 81.0826 - rmse: 99.3200 - val_loss: 66.8467 - val_rmse: 82.1156\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 66.4291 - rmse: 83.0912 - val_loss: 79.3265 - val_rmse: 100.5850\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 23s 469ms/step - loss: 67.9431 - rmse: 88.0528 - val_loss: 92.5849 - val_rmse: 114.3253\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 56.6549 - rmse: 73.2301 - val_loss: 52.4745 - val_rmse: 69.1241\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 51.0834 - rmse: 66.9367 - val_loss: 48.1687 - val_rmse: 60.6774\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 45.5672 - rmse: 59.5856 - val_loss: 57.9534 - val_rmse: 75.5423\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 43.4399 - rmse: 57.1646 - val_loss: 36.2296 - val_rmse: 46.7996\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 41.8251 - rmse: 54.8100 - val_loss: 39.0455 - val_rmse: 52.5171\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 39.8952 - rmse: 52.6174 - val_loss: 45.5384 - val_rmse: 60.5097\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 8s 157ms/step - loss: 37.6380 - rmse: 49.6575 - val_loss: 39.3155 - val_rmse: 52.6547\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 36.6624 - rmse: 48.1511 - val_loss: 32.7635 - val_rmse: 43.7048\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 8s 157ms/step - loss: 34.5032 - rmse: 45.7704 - val_loss: 38.9413 - val_rmse: 51.6873\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 33.6086 - rmse: 44.5877 - val_loss: 32.9324 - val_rmse: 43.1079\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 33.4487 - rmse: 44.4674 - val_loss: 36.9803 - val_rmse: 48.2113\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 31.8561 - rmse: 42.1974 - val_loss: 41.7295 - val_rmse: 53.5173\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 31.1339 - rmse: 41.4185 - val_loss: 46.1435 - val_rmse: 60.7780\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 30.0051 - rmse: 40.2103 - val_loss: 35.2239 - val_rmse: 46.5239\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 30.3681 - rmse: 40.4540 - val_loss: 33.2029 - val_rmse: 43.7656\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 28.2216 - rmse: 37.8059 - val_loss: 36.2318 - val_rmse: 47.8178\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 29.1625 - rmse: 38.9002 - val_loss: 31.6268 - val_rmse: 41.7946\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 28.4544 - rmse: 38.2346 - val_loss: 38.9679 - val_rmse: 50.5324\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 27.6283 - rmse: 37.2845 - val_loss: 28.6130 - val_rmse: 38.3694\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 27.3978 - rmse: 36.9756 - val_loss: 30.6756 - val_rmse: 40.9594\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 27.1937 - rmse: 36.6870 - val_loss: 30.4269 - val_rmse: 40.4247\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 86.2038 - rmse: 105.8551 - val_loss: 62.6340 - val_rmse: 76.5877\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 68.1506 - rmse: 84.1086 - val_loss: 54.6725 - val_rmse: 67.7436\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 55.0233 - rmse: 70.0987 - val_loss: 61.1123 - val_rmse: 75.7192\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 47.5312 - rmse: 61.7052 - val_loss: 44.4304 - val_rmse: 55.5045\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 41.8938 - rmse: 55.0132 - val_loss: 40.3142 - val_rmse: 53.0771\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 10s 199ms/step - loss: 40.2032 - rmse: 53.0272 - val_loss: 40.0604 - val_rmse: 51.1840\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 10s 203ms/step - loss: 38.7068 - rmse: 51.5670 - val_loss: 39.5454 - val_rmse: 52.7974\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 36.1875 - rmse: 48.2602 - val_loss: 33.8370 - val_rmse: 45.2385\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 35.9770 - rmse: 48.0432 - val_loss: 33.3867 - val_rmse: 44.6391\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 34.5876 - rmse: 46.4307 - val_loss: 34.4724 - val_rmse: 46.1013\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 33.3830 - rmse: 44.4691 - val_loss: 36.9848 - val_rmse: 49.2637\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 34.0701 - rmse: 45.3277 - val_loss: 30.9926 - val_rmse: 41.4203\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 32.3270 - rmse: 43.4468 - val_loss: 38.0386 - val_rmse: 50.0107\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 32.0993 - rmse: 42.7834 - val_loss: 36.3891 - val_rmse: 47.4047\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 31.6352 - rmse: 42.2039 - val_loss: 33.2231 - val_rmse: 43.5999\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 30.9397 - rmse: 41.2991 - val_loss: 36.6418 - val_rmse: 49.1345\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 28.3391 - rmse: 38.2027 - val_loss: 31.4135 - val_rmse: 41.3900\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 11s 211ms/step - loss: 27.2504 - rmse: 36.9571 - val_loss: 33.2931 - val_rmse: 43.6593\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 11s 224ms/step - loss: 27.3829 - rmse: 37.0187 - val_loss: 28.6858 - val_rmse: 37.8132\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 27.5462 - rmse: 37.1280 - val_loss: 27.5303 - val_rmse: 36.6437\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 11s 218ms/step - loss: 26.5738 - rmse: 35.8132 - val_loss: 29.4890 - val_rmse: 39.0392\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 11s 222ms/step - loss: 26.7537 - rmse: 36.2801 - val_loss: 30.9080 - val_rmse: 40.8326\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 17s 334ms/step - loss: 138.2796 - rmse: 167.1338 - val_loss: 60.8282 - val_rmse: 74.5163\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 11s 223ms/step - loss: 67.0113 - rmse: 83.1151 - val_loss: 82.6726 - val_rmse: 102.8811\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 221ms/step - loss: 57.7722 - rmse: 73.6711 - val_loss: 44.9753 - val_rmse: 57.7376\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 11s 212ms/step - loss: 48.9003 - rmse: 63.5296 - val_loss: 45.7104 - val_rmse: 58.3288\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 43.6156 - rmse: 57.2289 - val_loss: 39.9535 - val_rmse: 53.2822\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 221ms/step - loss: 41.0426 - rmse: 54.2514 - val_loss: 40.9761 - val_rmse: 52.4658\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 38.9619 - rmse: 51.5387 - val_loss: 34.5452 - val_rmse: 46.0667\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 11s 221ms/step - loss: 37.2725 - rmse: 49.6779 - val_loss: 34.2418 - val_rmse: 45.3566\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 11s 218ms/step - loss: 31.4433 - rmse: 42.1826 - val_loss: 30.1958 - val_rmse: 40.0580\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 11s 226ms/step - loss: 31.4265 - rmse: 41.7030 - val_loss: 30.0486 - val_rmse: 39.4496\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 29.3244 - rmse: 39.4627 - val_loss: 39.0540 - val_rmse: 49.9850\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 11s 214ms/step - loss: 29.0002 - rmse: 39.0189 - val_loss: 32.8348 - val_rmse: 42.2554\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 11s 219ms/step - loss: 29.2990 - rmse: 39.3428 - val_loss: 37.5204 - val_rmse: 49.5566\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 28.0805 - rmse: 37.6638 - val_loss: 31.9271 - val_rmse: 42.0104\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 28.0521 - rmse: 37.8961 - val_loss: 34.5334 - val_rmse: 45.9068\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 11s 220ms/step - loss: 27.4781 - rmse: 36.8715 - val_loss: 32.0065 - val_rmse: 42.2499\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 26.7399 - rmse: 36.0828 - val_loss: 30.9838 - val_rmse: 40.4671\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 27.5061 - rmse: 36.8541 - val_loss: 36.3762 - val_rmse: 47.6984\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 11s 218ms/step - loss: 26.1804 - rmse: 35.5234 - val_loss: 28.9911 - val_rmse: 38.7719\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 11s 224ms/step - loss: 26.1373 - rmse: 35.4608 - val_loss: 27.4934 - val_rmse: 36.9741\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 17s 349ms/step - loss: 93.1596 - rmse: 113.8618 - val_loss: 61.2599 - val_rmse: 76.7352\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 65.3051 - rmse: 82.2441 - val_loss: 46.6896 - val_rmse: 59.8068\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 32.6890 - rmse: 43.5698 - val_loss: 31.1781 - val_rmse: 41.3151\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 31.6987 - rmse: 42.4703 - val_loss: 33.6545 - val_rmse: 44.3228\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 31.4058 - rmse: 42.2571 - val_loss: 34.7530 - val_rmse: 44.3569\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 30.5974 - rmse: 41.2399 - val_loss: 38.2368 - val_rmse: 49.7158\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 11s 227ms/step - loss: 30.0157 - rmse: 40.5220 - val_loss: 31.4435 - val_rmse: 41.7736\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 29.6816 - rmse: 40.1605 - val_loss: 35.9374 - val_rmse: 46.5100\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 29.9365 - rmse: 40.2542 - val_loss: 30.5574 - val_rmse: 40.3966\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 10s 197ms/step - loss: 28.4677 - rmse: 38.5459 - val_loss: 32.4144 - val_rmse: 42.2586\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 10s 201ms/step - loss: 28.9446 - rmse: 39.3726 - val_loss: 29.1962 - val_rmse: 39.0143\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 14s 282ms/step - loss: 28.1461 - rmse: 38.2798 - val_loss: 35.4503 - val_rmse: 45.7204\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 26s 514ms/step - loss: 28.0273 - rmse: 37.8153 - val_loss: 33.9717 - val_rmse: 44.9116\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 22s 430ms/step - loss: 28.0330 - rmse: 37.9203 - val_loss: 28.0288 - val_rmse: 37.6889\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 18s 352ms/step - loss: 99.8206 - rmse: 121.1137 - val_loss: 65.7180 - val_rmse: 79.7026\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 69.7279 - rmse: 86.1909 - val_loss: 62.7788 - val_rmse: 80.1804\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 25.6852 - rmse: 34.8890 - val_loss: 28.3964 - val_rmse: 37.6841\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 25.4385 - rmse: 34.5326 - val_loss: 33.6906 - val_rmse: 44.2229\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 24.8829 - rmse: 33.7727 - val_loss: 28.4556 - val_rmse: 38.1399\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 250ms/step - loss: 24.9260 - rmse: 33.8704 - val_loss: 29.3179 - val_rmse: 38.7710\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 86.3738 - rmse: 105.6912 - val_loss: 61.9938 - val_rmse: 76.7667\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 65.8410 - rmse: 81.8438 - val_loss: 60.8599 - val_rmse: 78.3950\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 49.4370 - rmse: 63.8724 - val_loss: 65.7708 - val_rmse: 81.6662\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 45.4100 - rmse: 59.4221 - val_loss: 40.4156 - val_rmse: 51.7819\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 40.3617 - rmse: 53.1859 - val_loss: 45.3416 - val_rmse: 60.6247\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 12s 250ms/step - loss: 39.2600 - rmse: 51.7016 - val_loss: 41.5474 - val_rmse: 53.2451\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 36.3261 - rmse: 48.2709 - val_loss: 35.5195 - val_rmse: 47.4180\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 35.1049 - rmse: 46.8576 - val_loss: 32.9024 - val_rmse: 43.8598\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 34.0054 - rmse: 45.3391 - val_loss: 32.1057 - val_rmse: 42.2581\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 21s 420ms/step - loss: 358.4247 - rmse: 461.4482 - val_loss: 101.7280 - val_rmse: 125.2607\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 13s 263ms/step - loss: 82.6402 - rmse: 100.8952 - val_loss: 65.0252 - val_rmse: 79.1578\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 70.0551 - rmse: 86.0399 - val_loss: 59.5194 - val_rmse: 74.0226\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 63.4226 - rmse: 80.0980 - val_loss: 62.7691 - val_rmse: 75.4628\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 14s 271ms/step - loss: 51.2246 - rmse: 66.0139 - val_loss: 50.0786 - val_rmse: 67.3713\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 45.2373 - rmse: 59.1965 - val_loss: 45.9063 - val_rmse: 57.3732\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 41.9277 - rmse: 54.8920 - val_loss: 38.3918 - val_rmse: 50.5286\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 40.3414 - rmse: 53.3878 - val_loss: 35.1274 - val_rmse: 46.3241\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 10s 208ms/step - loss: 37.2931 - rmse: 49.6967 - val_loss: 45.4350 - val_rmse: 59.4173\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 37.6029 - rmse: 49.8899 - val_loss: 41.0080 - val_rmse: 54.3049\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 35.7175 - rmse: 47.3489 - val_loss: 35.2278 - val_rmse: 46.6254\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 45.6155 - rmse: 59.8511 - val_loss: 45.6337 - val_rmse: 57.0427\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 40.9118 - rmse: 53.7178 - val_loss: 35.8894 - val_rmse: 47.0644\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 38.9385 - rmse: 51.3776 - val_loss: 36.2178 - val_rmse: 47.9325\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 13s 257ms/step - loss: 37.1108 - rmse: 49.3251 - val_loss: 34.6955 - val_rmse: 45.9204\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 14s 273ms/step - loss: 35.4806 - rmse: 47.0387 - val_loss: 41.9147 - val_rmse: 55.7969\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 13s 252ms/step - loss: 33.7652 - rmse: 44.9306 - val_loss: 37.9639 - val_rmse: 50.3739\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 13s 267ms/step - loss: 32.8549 - rmse: 43.8531 - val_loss: 33.6009 - val_rmse: 45.2015\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 18s 360ms/step - loss: 33.0297 - rmse: 43.7883 - val_loss: 38.4421 - val_rmse: 50.7115\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 17s 331ms/step - loss: 31.6123 - rmse: 42.3589 - val_loss: 30.1577 - val_rmse: 40.0672\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 14s 282ms/step - loss: 31.3359 - rmse: 41.9684 - val_loss: 35.6415 - val_rmse: 45.9575\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 30.7191 - rmse: 40.9491 - val_loss: 35.0753 - val_rmse: 46.5668\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 29.8801 - rmse: 40.0165 - val_loss: 29.9940 - val_rmse: 39.6585\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 13s 261ms/step - loss: 35.8107 - rmse: 47.9245 - val_loss: 47.2919 - val_rmse: 61.9625\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 13s 260ms/step - loss: 35.5140 - rmse: 47.2167 - val_loss: 33.4602 - val_rmse: 44.7668\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 13s 257ms/step - loss: 33.9080 - rmse: 45.1931 - val_loss: 44.1228 - val_rmse: 57.2799\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 13s 251ms/step - loss: 34.5823 - rmse: 45.9056 - val_loss: 33.5962 - val_rmse: 43.8867\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 32.9549 - rmse: 43.9580 - val_loss: 30.5719 - val_rmse: 40.3459\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 13s 262ms/step - loss: 32.0373 - rmse: 42.9084 - val_loss: 31.8886 - val_rmse: 42.6104\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 30.6929 - rmse: 40.9637 - val_loss: 40.3451 - val_rmse: 52.5083\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 30.5509 - rmse: 40.9616 - val_loss: 35.6987 - val_rmse: 46.7683\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 29.7385 - rmse: 39.8602 - val_loss: 30.6785 - val_rmse: 39.9479\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 29.7525 - rmse: 39.9280 - val_loss: 30.3136 - val_rmse: 40.6748\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 8s 162ms/step - loss: 32.7397 - rmse: 43.7102 - val_loss: 38.8086 - val_rmse: 51.4171\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 31.8001 - rmse: 42.5070 - val_loss: 39.0617 - val_rmse: 52.0376\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 31.1232 - rmse: 41.7985 - val_loss: 49.2260 - val_rmse: 64.0121\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 30.9205 - rmse: 41.3443 - val_loss: 52.4571 - val_rmse: 66.5278\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 30.3205 - rmse: 40.5709 - val_loss: 32.4831 - val_rmse: 42.2534\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 30.0237 - rmse: 40.1092 - val_loss: 31.3069 - val_rmse: 41.0876\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 188ms/step - loss: 29.4049 - rmse: 39.2771 - val_loss: 39.6581 - val_rmse: 51.2136\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 29.6088 - rmse: 39.5792 - val_loss: 28.3187 - val_rmse: 37.6995\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 28.3900 - rmse: 38.2635 - val_loss: 34.4774 - val_rmse: 44.7639\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 27.7743 - rmse: 37.5016 - val_loss: 27.9787 - val_rmse: 37.4522\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 27.8991 - rmse: 37.4308 - val_loss: 31.3875 - val_rmse: 41.4074\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 18s 367ms/step - loss: 2950.7736 - rmse: 4088.4463 - val_loss: 1034.3948 - val_rmse: 1829.6853\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 9s 186ms/step - loss: 343.7619 - rmse: 777.6334 - val_loss: 114.5861 - val_rmse: 143.7565\n",
      "50/50 [==============================] - 10s 199ms/step - loss: 30.2939 - rmse: 40.5604 - val_loss: 38.0576 - val_rmse: 50.1534\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 11s 220ms/step - loss: 29.9307 - rmse: 40.0943 - val_loss: 32.7253 - val_rmse: 43.4582\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 17s 336ms/step - loss: 132.4325 - rmse: 162.8141 - val_loss: 64.8530 - val_rmse: 80.4480\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 75.1459 - rmse: 93.2036 - val_loss: 85.0673 - val_rmse: 105.1578\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 70.9052 - rmse: 88.5188 - val_loss: 60.4745 - val_rmse: 76.8337\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 58.1534 - rmse: 74.0166 - val_loss: 49.1044 - val_rmse: 62.0797\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 51.3696 - rmse: 66.3537 - val_loss: 69.9601 - val_rmse: 88.1153\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 48.7247 - rmse: 63.1754 - val_loss: 43.0061 - val_rmse: 53.9186\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 8s 162ms/step - loss: 45.1609 - rmse: 58.8264 - val_loss: 38.3832 - val_rmse: 50.6118\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 42.4730 - rmse: 55.8030 - val_loss: 38.2307 - val_rmse: 50.6186\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 41.8572 - rmse: 55.2178 - val_loss: 38.0600 - val_rmse: 50.6144\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 39.7128 - rmse: 52.5537 - val_loss: 47.2117 - val_rmse: 61.9604\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 37.8339 - rmse: 50.0806 - val_loss: 36.5933 - val_rmse: 48.8439\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 40.1385 - rmse: 53.0399 - val_loss: 34.4224 - val_rmse: 46.2835\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 38.6047 - rmse: 50.9302 - val_loss: 33.9733 - val_rmse: 45.7442\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 36.3586 - rmse: 48.0282 - val_loss: 48.8154 - val_rmse: 63.7357\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 37.0657 - rmse: 49.0354 - val_loss: 38.7139 - val_rmse: 51.3444\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 35.7332 - rmse: 47.0871 - val_loss: 33.4424 - val_rmse: 44.6904\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 34.9416 - rmse: 46.1202 - val_loss: 45.0409 - val_rmse: 58.5916\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 33.4638 - rmse: 44.3480 - val_loss: 34.6577 - val_rmse: 45.7623\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 33.2561 - rmse: 44.4333 - val_loss: 32.0189 - val_rmse: 41.9965\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 32.2431 - rmse: 42.8910 - val_loss: 35.5458 - val_rmse: 47.2968\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 32.3116 - rmse: 43.1426 - val_loss: 34.1675 - val_rmse: 43.8480\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 31.4149 - rmse: 41.8396 - val_loss: 32.0344 - val_rmse: 41.9177\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 30.7055 - rmse: 41.2060 - val_loss: 35.0118 - val_rmse: 46.3813\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 30.6001 - rmse: 40.8475 - val_loss: 30.8200 - val_rmse: 41.1120\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 187ms/step - loss: 29.3655 - rmse: 39.4065 - val_loss: 30.9586 - val_rmse: 40.9061\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 28.8233 - rmse: 38.6240 - val_loss: 29.9007 - val_rmse: 39.0891\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 27.9549 - rmse: 37.6975 - val_loss: 30.9690 - val_rmse: 41.0156\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 28.4730 - rmse: 38.2750 - val_loss: 28.5701 - val_rmse: 37.7412\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 27.9072 - rmse: 37.5418 - val_loss: 32.4009 - val_rmse: 43.0655\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 26.8838 - rmse: 36.4191 - val_loss: 30.1959 - val_rmse: 40.0564\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 20s 403ms/step - loss: 102.0921 - rmse: 124.6693 - val_loss: 95.5425 - val_rmse: 116.1122\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 85.7855 - rmse: 105.1772 - val_loss: 60.4500 - val_rmse: 73.8329\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 14s 273ms/step - loss: 67.5749 - rmse: 83.9175 - val_loss: 50.7968 - val_rmse: 64.3742\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 13s 252ms/step - loss: 53.9674 - rmse: 69.1273 - val_loss: 60.3425 - val_rmse: 76.8444\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 13s 266ms/step - loss: 49.7785 - rmse: 63.9656 - val_loss: 61.7266 - val_rmse: 75.2680\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 46.5306 - rmse: 60.5383 - val_loss: 44.4636 - val_rmse: 55.9640\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 13s 263ms/step - loss: 42.4396 - rmse: 55.6585 - val_loss: 43.0224 - val_rmse: 56.6029\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 28.8649 - rmse: 38.6529 - val_loss: 32.5071 - val_rmse: 43.3154\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 27.6156 - rmse: 37.0609 - val_loss: 33.4826 - val_rmse: 43.7707\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 27.9590 - rmse: 37.4341 - val_loss: 35.0355 - val_rmse: 45.6512\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 26.6229 - rmse: 35.8313 - val_loss: 31.5831 - val_rmse: 41.7101\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 26.2480 - rmse: 35.4640 - val_loss: 30.0101 - val_rmse: 40.0236\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 107.5949 - rmse: 132.5995 - val_loss: 67.2617 - val_rmse: 80.7578\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 76.4054 - rmse: 94.0879 - val_loss: 68.3798 - val_rmse: 86.2337\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 66.0913 - rmse: 82.5972 - val_loss: 52.2632 - val_rmse: 65.5247\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 54.4227 - rmse: 69.6177 - val_loss: 44.7649 - val_rmse: 57.6072\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 47.8840 - rmse: 62.0553 - val_loss: 40.5708 - val_rmse: 52.1615\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 44.3828 - rmse: 58.3810 - val_loss: 39.7199 - val_rmse: 51.8080\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 42.4958 - rmse: 56.0930 - val_loss: 37.4078 - val_rmse: 49.2442\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 41.2475 - rmse: 55.2463 - val_loss: 40.1936 - val_rmse: 52.4375\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 10s 190ms/step - loss: 39.6569 - rmse: 52.4622 - val_loss: 36.1649 - val_rmse: 49.1095\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 38.0372 - rmse: 50.5782 - val_loss: 36.1885 - val_rmse: 48.8809\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 10s 199ms/step - loss: 37.3511 - rmse: 49.6187 - val_loss: 33.6090 - val_rmse: 45.2764\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 43.4672 - rmse: 56.6380 - val_loss: 55.1936 - val_rmse: 71.1790\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 40.4521 - rmse: 53.2286 - val_loss: 39.1068 - val_rmse: 51.9043\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 10s 202ms/step - loss: 39.0328 - rmse: 51.4884 - val_loss: 35.5462 - val_rmse: 47.4950\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 10s 205ms/step - loss: 37.6621 - rmse: 49.5978 - val_loss: 37.4346 - val_rmse: 50.5707\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 37.3696 - rmse: 49.3373 - val_loss: 33.3099 - val_rmse: 44.3230\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 10s 201ms/step - loss: 34.3989 - rmse: 45.9656 - val_loss: 33.9098 - val_rmse: 44.8414\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 34.5794 - rmse: 46.0764 - val_loss: 36.5018 - val_rmse: 48.3569\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 10s 203ms/step - loss: 33.2290 - rmse: 44.1232 - val_loss: 34.9687 - val_rmse: 45.9831\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 10s 202ms/step - loss: 32.1839 - rmse: 42.9103 - val_loss: 36.1128 - val_rmse: 46.7033\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 10s 192ms/step - loss: 32.5759 - rmse: 43.1552 - val_loss: 33.1284 - val_rmse: 43.1261\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 10s 209ms/step - loss: 30.5916 - rmse: 40.7141 - val_loss: 42.9997 - val_rmse: 55.4544\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 10s 196ms/step - loss: 29.9835 - rmse: 39.8111 - val_loss: 35.4474 - val_rmse: 45.9494\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 29.2353 - rmse: 38.9765 - val_loss: 32.4733 - val_rmse: 42.2974\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 34.5620 - rmse: 45.8480 - val_loss: 32.5463 - val_rmse: 42.8374\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 11s 227ms/step - loss: 34.0744 - rmse: 44.8523 - val_loss: 39.9185 - val_rmse: 51.5231\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 11s 221ms/step - loss: 32.8352 - rmse: 43.5874 - val_loss: 36.1713 - val_rmse: 46.4171\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 32.4360 - rmse: 43.1630 - val_loss: 38.3496 - val_rmse: 49.1936\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 11s 211ms/step - loss: 31.7523 - rmse: 42.1981 - val_loss: 38.2451 - val_rmse: 48.8928\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 10s 190ms/step - loss: 31.0993 - rmse: 41.3781 - val_loss: 38.0273 - val_rmse: 48.9982\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 13s 252ms/step - loss: 30.9845 - rmse: 41.1261 - val_loss: 31.3050 - val_rmse: 41.2406\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 13s 264ms/step - loss: 29.9863 - rmse: 39.9038 - val_loss: 31.7508 - val_rmse: 41.2371\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 14s 278ms/step - loss: 29.3450 - rmse: 39.3367 - val_loss: 30.7507 - val_rmse: 40.6999\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 29.0931 - rmse: 38.8117 - val_loss: 31.2200 - val_rmse: 40.9852\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 18s 353ms/step - loss: 28.1866 - rmse: 37.8308 - val_loss: 39.9214 - val_rmse: 50.9921\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 14s 279ms/step - loss: 28.1883 - rmse: 37.7442 - val_loss: 28.5570 - val_rmse: 38.1126\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 16s 328ms/step - loss: 118.7656 - rmse: 144.1653 - val_loss: 119.8001 - val_rmse: 142.4977\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 10s 197ms/step - loss: 80.8154 - rmse: 99.6700 - val_loss: 128.6942 - val_rmse: 151.0817\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 74.9855 - rmse: 92.5367 - val_loss: 65.4161 - val_rmse: 82.5660\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 68.5595 - rmse: 84.9080 - val_loss: 68.0457 - val_rmse: 86.3523\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 58.5659 - rmse: 73.6700 - val_loss: 94.0399 - val_rmse: 113.9816\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 10s 204ms/step - loss: 51.0961 - rmse: 65.1362 - val_loss: 52.4249 - val_rmse: 67.6039\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 46.9992 - rmse: 60.6516 - val_loss: 79.3607 - val_rmse: 97.0551\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 45.5589 - rmse: 59.6432 - val_loss: 58.3603 - val_rmse: 75.7588\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 42.7918 - rmse: 56.3289 - val_loss: 53.4802 - val_rmse: 69.7042\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 40.7599 - rmse: 53.8703 - val_loss: 64.9018 - val_rmse: 81.5729\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 40.1040 - rmse: 53.1243 - val_loss: 67.4139 - val_rmse: 84.3265\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 38.1159 - rmse: 50.5644 - val_loss: 48.2914 - val_rmse: 63.1224\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 13s 268ms/step - loss: 31.1399 - rmse: 41.8131 - val_loss: 46.9718 - val_rmse: 59.7661\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 13s 260ms/step - loss: 32.5861 - rmse: 43.3208 - val_loss: 33.1141 - val_rmse: 43.0807\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 14s 279ms/step - loss: 30.5224 - rmse: 41.1004 - val_loss: 32.9378 - val_rmse: 43.2924\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 31.1409 - rmse: 41.7397 - val_loss: 34.5432 - val_rmse: 45.3192\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 13s 265ms/step - loss: 29.6051 - rmse: 39.9166 - val_loss: 31.1648 - val_rmse: 41.5533\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 27s 546ms/step - loss: 107.8398 - rmse: 131.0184 - val_loss: 108.4636 - val_rmse: 130.6646\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 18s 360ms/step - loss: 79.2450 - rmse: 97.6034 - val_loss: 78.3630 - val_rmse: 97.1389\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 62.0245 - rmse: 77.9861 - val_loss: 76.8570 - val_rmse: 91.7212\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 17s 346ms/step - loss: 50.8125 - rmse: 65.4593 - val_loss: 42.0561 - val_rmse: 53.7967\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 18s 357ms/step - loss: 49.0012 - rmse: 63.7245 - val_loss: 58.7752 - val_rmse: 75.2437\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 43.7367 - rmse: 57.4409 - val_loss: 48.0233 - val_rmse: 59.4060\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 41.2534 - rmse: 54.4590 - val_loss: 39.4456 - val_rmse: 51.5977\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 38.5783 - rmse: 51.2402 - val_loss: 32.2514 - val_rmse: 42.8253\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 17s 350ms/step - loss: 36.5432 - rmse: 48.5414 - val_loss: 35.3525 - val_rmse: 46.8712\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 18s 357ms/step - loss: 36.8169 - rmse: 48.7148 - val_loss: 34.2750 - val_rmse: 46.0711\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 34.8844 - rmse: 46.4998 - val_loss: 35.5810 - val_rmse: 47.8737\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 34.1191 - rmse: 45.2620 - val_loss: 32.4158 - val_rmse: 43.6632\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 32.7239 - rmse: 43.5235 - val_loss: 37.1002 - val_rmse: 48.9357\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 17s 337ms/step - loss: 32.2887 - rmse: 43.0541 - val_loss: 30.0617 - val_rmse: 39.8797\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 30.5015 - rmse: 40.6967 - val_loss: 41.0535 - val_rmse: 52.4194\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 18s 370ms/step - loss: 30.5597 - rmse: 40.7708 - val_loss: 38.8127 - val_rmse: 51.5606\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 29.9933 - rmse: 40.3203 - val_loss: 35.8113 - val_rmse: 46.8900\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 18s 351ms/step - loss: 29.8339 - rmse: 39.8480 - val_loss: 35.6023 - val_rmse: 47.5809\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 17s 347ms/step - loss: 28.7134 - rmse: 38.5960 - val_loss: 33.4624 - val_rmse: 45.0204\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 27.8371 - rmse: 37.5298 - val_loss: 31.0636 - val_rmse: 41.7110\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 17s 347ms/step - loss: 27.5403 - rmse: 37.0702 - val_loss: 29.7818 - val_rmse: 40.0430\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 27.0449 - rmse: 36.3871 - val_loss: 33.3068 - val_rmse: 43.0931\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 26.4736 - rmse: 35.7653 - val_loss: 30.4504 - val_rmse: 41.2271\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 14s 282ms/step - loss: 26.0574 - rmse: 35.1252 - val_loss: 29.4576 - val_rmse: 39.0000\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 17s 348ms/step - loss: 25.7933 - rmse: 34.9226 - val_loss: 28.1025 - val_rmse: 37.7891\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 114.8012 - rmse: 139.1710 - val_loss: 61.4558 - val_rmse: 75.1523\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 17s 347ms/step - loss: 79.7755 - rmse: 98.2525 - val_loss: 91.9064 - val_rmse: 112.3426\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 71.7277 - rmse: 89.6963 - val_loss: 56.1324 - val_rmse: 72.6178\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 60.8628 - rmse: 81.2059 - val_loss: 44.8387 - val_rmse: 58.4985\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 50.2546 - rmse: 64.5226 - val_loss: 48.1847 - val_rmse: 63.3994\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 45.5098 - rmse: 59.0809 - val_loss: 51.5926 - val_rmse: 66.4587\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 43.2211 - rmse: 56.4589 - val_loss: 52.6636 - val_rmse: 67.5754\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 17s 332ms/step - loss: 40.7915 - rmse: 53.9302 - val_loss: 36.9051 - val_rmse: 47.3817\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 39.6340 - rmse: 51.9584 - val_loss: 34.1237 - val_rmse: 45.3240\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 37.4724 - rmse: 49.3495 - val_loss: 34.6038 - val_rmse: 46.4354\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 35.6344 - rmse: 47.3339 - val_loss: 35.8078 - val_rmse: 47.9667\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 17s 336ms/step - loss: 35.3137 - rmse: 46.7662 - val_loss: 33.9270 - val_rmse: 44.6769\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 17s 335ms/step - loss: 33.1732 - rmse: 44.2054 - val_loss: 43.4013 - val_rmse: 57.2890\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 16s 328ms/step - loss: 32.7695 - rmse: 43.5984 - val_loss: 31.8184 - val_rmse: 42.1388\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 17s 349ms/step - loss: 31.1669 - rmse: 41.7686 - val_loss: 33.6487 - val_rmse: 44.2542\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 17s 335ms/step - loss: 32.1723 - rmse: 42.7877 - val_loss: 41.4777 - val_rmse: 54.8048\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 14s 287ms/step - loss: 30.0898 - rmse: 40.1994 - val_loss: 33.9875 - val_rmse: 44.6718\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 30.4030 - rmse: 40.7684 - val_loss: 32.3463 - val_rmse: 42.8260\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 29.6699 - rmse: 39.5341 - val_loss: 31.9654 - val_rmse: 42.1077\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 17s 334ms/step - loss: 29.3640 - rmse: 39.2305 - val_loss: 28.9205 - val_rmse: 39.2822\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 18s 361ms/step - loss: 27.9797 - rmse: 37.7872 - val_loss: 36.4097 - val_rmse: 47.5122\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 26.9681 - rmse: 36.4543 - val_loss: 29.7927 - val_rmse: 39.2765\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 14s 287ms/step - loss: 27.4075 - rmse: 36.9822 - val_loss: 34.2136 - val_rmse: 44.5981\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 14s 284ms/step - loss: 26.3636 - rmse: 35.8887 - val_loss: 31.9960 - val_rmse: 42.0729\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 65.9965 - rmse: 84.1151 - val_loss: 90.8923 - val_rmse: 112.8590\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 17s 349ms/step - loss: 55.4563 - rmse: 71.2620 - val_loss: 51.6834 - val_rmse: 64.1914\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 50.8273 - rmse: 66.7908 - val_loss: 60.8858 - val_rmse: 77.6227\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 56.4166 - rmse: 74.4852 - val_loss: 43.2251 - val_rmse: 57.6908\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 15s 306ms/step - loss: 48.2784 - rmse: 63.1245 - val_loss: 40.5862 - val_rmse: 54.4923\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 17s 330ms/step - loss: 45.9388 - rmse: 60.1518 - val_loss: 46.2776 - val_rmse: 61.9681\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 43.0784 - rmse: 56.9172 - val_loss: 35.5673 - val_rmse: 47.4634\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 17s 348ms/step - loss: 41.0317 - rmse: 54.2086 - val_loss: 33.7214 - val_rmse: 44.8333\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 38.3772 - rmse: 51.1385 - val_loss: 44.5189 - val_rmse: 58.0498\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 39.2849 - rmse: 52.3083 - val_loss: 39.9362 - val_rmse: 50.9810\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 18s 351ms/step - loss: 37.1040 - rmse: 49.1409 - val_loss: 35.3162 - val_rmse: 45.4655\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 19s 376ms/step - loss: 35.2745 - rmse: 46.7651 - val_loss: 46.6295 - val_rmse: 60.6455\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 34.4484 - rmse: 45.7723 - val_loss: 41.7011 - val_rmse: 53.6812\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 33.8135 - rmse: 44.8726 - val_loss: 40.2234 - val_rmse: 52.3511\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 18s 359ms/step - loss: 33.3769 - rmse: 44.5532 - val_loss: 35.7180 - val_rmse: 46.5524\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 18s 370ms/step - loss: 32.3255 - rmse: 43.2548 - val_loss: 30.8189 - val_rmse: 40.9018\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 18s 367ms/step - loss: 31.4158 - rmse: 42.1803 - val_loss: 38.6457 - val_rmse: 49.4201\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 32.7294 - rmse: 43.7594 - val_loss: 35.1666 - val_rmse: 45.7444\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 31.9287 - rmse: 42.4716 - val_loss: 32.5803 - val_rmse: 42.5468\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 30.5701 - rmse: 41.0688 - val_loss: 34.0472 - val_rmse: 43.7166\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 31.0023 - rmse: 41.5568 - val_loss: 34.1807 - val_rmse: 45.2000\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 28.9866 - rmse: 39.2220 - val_loss: 47.6485 - val_rmse: 60.9806\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 28.6055 - rmse: 38.7447 - val_loss: 34.9393 - val_rmse: 45.7948\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 28.5416 - rmse: 38.8760 - val_loss: 41.7060 - val_rmse: 54.8370\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 28.2973 - rmse: 38.1276 - val_loss: 32.4202 - val_rmse: 43.6476\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 27.4586 - rmse: 37.2502 - val_loss: 32.8221 - val_rmse: 43.3187\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 26.6110 - rmse: 36.1762 - val_loss: 31.0843 - val_rmse: 40.3216\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 26.3898 - rmse: 35.9970 - val_loss: 32.1020 - val_rmse: 42.8729\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 33.9561 - rmse: 45.4283 - val_loss: 41.4267 - val_rmse: 53.9326\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 33.3470 - rmse: 44.4773 - val_loss: 31.1185 - val_rmse: 41.2313\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 32.5170 - rmse: 43.2535 - val_loss: 29.8041 - val_rmse: 38.9766\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 31.2953 - rmse: 41.6997 - val_loss: 47.5294 - val_rmse: 61.7228\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 30.1800 - rmse: 40.5416 - val_loss: 33.1796 - val_rmse: 44.5037\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 30.3047 - rmse: 40.5466 - val_loss: 37.6958 - val_rmse: 48.5624\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 28.7184 - rmse: 38.7938 - val_loss: 35.3876 - val_rmse: 46.0432\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 29.2152 - rmse: 39.2552 - val_loss: 29.2364 - val_rmse: 38.7817\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 28.2647 - rmse: 38.0250 - val_loss: 34.5005 - val_rmse: 46.5553\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 27.1431 - rmse: 36.8037 - val_loss: 32.5807 - val_rmse: 42.6089\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 26.7228 - rmse: 36.4287 - val_loss: 35.2960 - val_rmse: 46.1968\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 26.6372 - rmse: 36.0655 - val_loss: 29.1826 - val_rmse: 38.9632\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 29.6785 - rmse: 39.7809 - val_loss: 46.6835 - val_rmse: 60.3536\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 28.6735 - rmse: 38.4713 - val_loss: 32.1460 - val_rmse: 42.4918\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 28.7068 - rmse: 38.6773 - val_loss: 33.7374 - val_rmse: 45.1316\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 28.1172 - rmse: 37.8872 - val_loss: 35.1787 - val_rmse: 46.1714\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 27.6177 - rmse: 37.3113 - val_loss: 30.0829 - val_rmse: 39.2642\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 26.2955 - rmse: 35.5830 - val_loss: 35.3386 - val_rmse: 47.1107\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 27.1247 - rmse: 36.7997 - val_loss: 30.3565 - val_rmse: 39.6845\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 26.7767 - rmse: 36.1600 - val_loss: 27.4099 - val_rmse: 37.0745\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 282.1814 - rmse: 336.5930 - val_loss: 81.3976 - val_rmse: 96.5943\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 13s 251ms/step - loss: 74.3160 - rmse: 90.6514 - val_loss: 99.2075 - val_rmse: 120.0017\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 76.8127 - rmse: 94.9251 - val_loss: 99.8340 - val_rmse: 121.2441\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 28.6278 - rmse: 38.5938 - val_loss: 33.6562 - val_rmse: 44.4200\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 29.1582 - rmse: 39.0466 - val_loss: 33.6948 - val_rmse: 43.9331\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 28.1741 - rmse: 37.8671 - val_loss: 29.2968 - val_rmse: 39.4612\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 27.6456 - rmse: 37.3927 - val_loss: 37.7961 - val_rmse: 49.2498\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 27.3085 - rmse: 36.8006 - val_loss: 29.1253 - val_rmse: 39.4308\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 17s 334ms/step - loss: 1920.8604 - rmse: 2447.2612 - val_loss: 82.9213 - val_rmse: 99.8677\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 230ms/step - loss: 94.5893 - rmse: 114.8332 - val_loss: 94.4338 - val_rmse: 113.4669\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 82.7007 - rmse: 100.6490 - val_loss: 67.1519 - val_rmse: 83.1796\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 11s 230ms/step - loss: 69.5539 - rmse: 86.8420 - val_loss: 77.8451 - val_rmse: 92.4423\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 11s 230ms/step - loss: 55.1363 - rmse: 70.4851 - val_loss: 43.0828 - val_rmse: 55.3839\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 227ms/step - loss: 47.0575 - rmse: 61.0013 - val_loss: 66.8689 - val_rmse: 81.4438\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 79.8257 - rmse: 100.3568 - val_loss: 81.6635 - val_rmse: 104.4654\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 65.7961 - rmse: 83.7811 - val_loss: 92.6569 - val_rmse: 113.7957\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 54.3046 - rmse: 70.0467 - val_loss: 54.9920 - val_rmse: 72.4005\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 50.4242 - rmse: 65.7282 - val_loss: 49.3485 - val_rmse: 64.0206\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 44.7539 - rmse: 58.8769 - val_loss: 35.5853 - val_rmse: 46.8469\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 11s 227ms/step - loss: 42.4773 - rmse: 55.8171 - val_loss: 36.6485 - val_rmse: 48.1504\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 39.7280 - rmse: 52.2424 - val_loss: 44.8275 - val_rmse: 59.1086\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 36.9780 - rmse: 49.2249 - val_loss: 35.9923 - val_rmse: 48.2269\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 38.4009 - rmse: 50.8953 - val_loss: 34.5573 - val_rmse: 46.2075\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 35.5190 - rmse: 47.3966 - val_loss: 33.1154 - val_rmse: 44.4590\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 11s 226ms/step - loss: 34.2528 - rmse: 45.6578 - val_loss: 36.2257 - val_rmse: 45.7351\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 33.0721 - rmse: 43.7560 - val_loss: 36.5339 - val_rmse: 48.1136\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 32.0731 - rmse: 42.5540 - val_loss: 32.4604 - val_rmse: 42.1975\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 43.6771 - rmse: 56.6139 - val_loss: 43.1607 - val_rmse: 57.1158\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 42.6635 - rmse: 55.5152 - val_loss: 34.8381 - val_rmse: 45.5456\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 38.6691 - rmse: 50.7053 - val_loss: 37.9426 - val_rmse: 49.8846\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 37.3063 - rmse: 49.1146 - val_loss: 46.4665 - val_rmse: 60.5326\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 34.8462 - rmse: 46.1535 - val_loss: 32.8424 - val_rmse: 43.6535\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 35.3090 - rmse: 46.5595 - val_loss: 31.7118 - val_rmse: 42.1750\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 33.8972 - rmse: 44.7482 - val_loss: 37.5305 - val_rmse: 49.7021\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 32.9471 - rmse: 43.5727 - val_loss: 30.4767 - val_rmse: 39.9720\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 31.7378 - rmse: 42.1866 - val_loss: 35.5904 - val_rmse: 45.9056\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 31.0299 - rmse: 41.2337 - val_loss: 32.9590 - val_rmse: 43.7024\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 29.5476 - rmse: 39.3126 - val_loss: 33.9946 - val_rmse: 43.9239\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 30.0393 - rmse: 39.8633 - val_loss: 28.5634 - val_rmse: 37.3587- loss: 3 - ETA: 1s - loss: 30.0997 -\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 13s 256ms/step - loss: 37.6120 - rmse: 49.9835 - val_loss: 45.5776 - val_rmse: 60.0876\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 35.9830 - rmse: 47.7460 - val_loss: 37.4124 - val_rmse: 49.8311\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 35.8829 - rmse: 47.6340 - val_loss: 36.6695 - val_rmse: 48.4671\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 34.1500 - rmse: 45.4972 - val_loss: 42.7778 - val_rmse: 55.4245\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 33.9040 - rmse: 45.0810 - val_loss: 36.0113 - val_rmse: 46.8867\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 32.9711 - rmse: 43.8641 - val_loss: 36.4071 - val_rmse: 47.8918\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 32.3376 - rmse: 42.9423 - val_loss: 35.7150 - val_rmse: 47.1068\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 30.4010 - rmse: 40.6244 - val_loss: 45.3781 - val_rmse: 58.9015\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 29.7375 - rmse: 39.6961 - val_loss: 35.0411 - val_rmse: 46.9037\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 29.8549 - rmse: 40.0651 - val_loss: 38.2191 - val_rmse: 49.8415\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 28.4886 - rmse: 38.2619 - val_loss: 31.1820 - val_rmse: 42.1183\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 27.9368 - rmse: 37.3685 - val_loss: 32.5059 - val_rmse: 43.2096\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 27.3201 - rmse: 36.8818 - val_loss: 28.7159 - val_rmse: 38.6861\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 27.8883 - rmse: 37.5694 - val_loss: 29.4126 - val_rmse: 39.4768\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 26.1534 - rmse: 35.3900 - val_loss: 28.0162 - val_rmse: 37.9502\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 26.8504 - rmse: 36.2149 - val_loss: 26.9825 - val_rmse: 36.4091\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 1394.7104 - rmse: 1807.3160 - val_loss: 97.2843 - val_rmse: 114.8960\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 85.8474 - rmse: 105.5538 - val_loss: 83.9336 - val_rmse: 101.9912\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 73.5145 - rmse: 91.7918 - val_loss: 59.8259 - val_rmse: 76.6548\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 58.7148 - rmse: 75.0239 - val_loss: 71.4444 - val_rmse: 90.0209\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 52.7212 - rmse: 68.4196 - val_loss: 79.4887 - val_rmse: 98.4828\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 230ms/step - loss: 47.4736 - rmse: 61.6701 - val_loss: 51.8602 - val_rmse: 68.1198\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 42.7587 - rmse: 56.1645 - val_loss: 46.1149 - val_rmse: 60.0119\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 42.7669 - rmse: 55.9942 - val_loss: 50.5786 - val_rmse: 65.4392\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 39.2557 - rmse: 51.5100 - val_loss: 54.9805 - val_rmse: 70.3856\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 38.7989 - rmse: 51.0426 - val_loss: 53.6198 - val_rmse: 69.3710\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 32.1606 - rmse: 42.7109 - val_loss: 46.7405 - val_rmse: 59.5259\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 31.6599 - rmse: 41.7383 - val_loss: 38.3169 - val_rmse: 48.4958\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 32.1528 - rmse: 42.7245 - val_loss: 49.2792 - val_rmse: 61.3041\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 30.9322 - rmse: 41.3457 - val_loss: 34.9572 - val_rmse: 44.1988\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 30.4125 - rmse: 40.4235 - val_loss: 36.1521 - val_rmse: 48.1661\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 230ms/step - loss: 29.8488 - rmse: 39.9299 - val_loss: 34.9480 - val_rmse: 45.8504\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 29.1354 - rmse: 39.1316 - val_loss: 30.3968 - val_rmse: 40.2193\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 28.1342 - rmse: 37.8764 - val_loss: 37.1348 - val_rmse: 47.9045\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 28.1242 - rmse: 37.7640 - val_loss: 29.0425 - val_rmse: 37.8779\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 230ms/step - loss: 27.9045 - rmse: 37.2402 - val_loss: 29.0752 - val_rmse: 38.4761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_dict = {\"epochs\":[25], \"layer_count\":[2, 3, 4, 5], \"node_number\":[160, 180, 200, 220], \"dropout\":[0.3, 0.7]}\n",
    "\n",
    "grid_search.search(feature_dict, unic2g_data, windows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(x):\n",
    "    return sum([splits[-1]*values['val_loss'] for (splits, values) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>layer_count</th>\n",
       "      <th>node_number</th>\n",
       "      <th>dropout</th>\n",
       "      <th>loss</th>\n",
       "      <th>rmse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.845056</td>\n",
       "      <td>33.734009</td>\n",
       "      <td>26.676975</td>\n",
       "      <td>36.386173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.494295</td>\n",
       "      <td>34.789776</td>\n",
       "      <td>26.759470</td>\n",
       "      <td>36.620213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26.824999</td>\n",
       "      <td>36.214920</td>\n",
       "      <td>26.982515</td>\n",
       "      <td>36.409119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.288883</td>\n",
       "      <td>38.149914</td>\n",
       "      <td>27.069027</td>\n",
       "      <td>36.188385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.7</td>\n",
       "      <td>27.182920</td>\n",
       "      <td>36.467155</td>\n",
       "      <td>27.242572</td>\n",
       "      <td>36.849945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26.703194</td>\n",
       "      <td>36.160015</td>\n",
       "      <td>27.409861</td>\n",
       "      <td>37.074490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26.116151</td>\n",
       "      <td>35.460846</td>\n",
       "      <td>27.493419</td>\n",
       "      <td>36.974094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.228306</td>\n",
       "      <td>34.108627</td>\n",
       "      <td>27.522308</td>\n",
       "      <td>37.175056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>26.567664</td>\n",
       "      <td>35.971237</td>\n",
       "      <td>27.772507</td>\n",
       "      <td>37.656372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.034410</td>\n",
       "      <td>37.920330</td>\n",
       "      <td>28.028752</td>\n",
       "      <td>37.688915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.7</td>\n",
       "      <td>26.723766</td>\n",
       "      <td>36.383057</td>\n",
       "      <td>28.099579</td>\n",
       "      <td>37.855316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.362162</td>\n",
       "      <td>34.922558</td>\n",
       "      <td>28.102494</td>\n",
       "      <td>37.789055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.6</td>\n",
       "      <td>29.156775</td>\n",
       "      <td>38.987270</td>\n",
       "      <td>28.160024</td>\n",
       "      <td>37.840137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.347189</td>\n",
       "      <td>38.367676</td>\n",
       "      <td>28.277698</td>\n",
       "      <td>38.089466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.720046</td>\n",
       "      <td>38.750923</td>\n",
       "      <td>28.379128</td>\n",
       "      <td>38.918564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.906912</td>\n",
       "      <td>35.073952</td>\n",
       "      <td>28.398543</td>\n",
       "      <td>38.345280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.278567</td>\n",
       "      <td>36.986805</td>\n",
       "      <td>28.417298</td>\n",
       "      <td>37.878738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6</td>\n",
       "      <td>28.427894</td>\n",
       "      <td>38.370934</td>\n",
       "      <td>28.488486</td>\n",
       "      <td>38.448299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.032149</td>\n",
       "      <td>37.744232</td>\n",
       "      <td>28.556961</td>\n",
       "      <td>38.112572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.197132</td>\n",
       "      <td>35.700542</td>\n",
       "      <td>28.750953</td>\n",
       "      <td>39.011215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.880777</td>\n",
       "      <td>36.623810</td>\n",
       "      <td>28.934041</td>\n",
       "      <td>38.583847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.8</td>\n",
       "      <td>27.524914</td>\n",
       "      <td>37.240166</td>\n",
       "      <td>29.075184</td>\n",
       "      <td>38.476078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.8</td>\n",
       "      <td>27.277222</td>\n",
       "      <td>36.800636</td>\n",
       "      <td>29.125298</td>\n",
       "      <td>39.430817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.685619</td>\n",
       "      <td>38.522980</td>\n",
       "      <td>29.193312</td>\n",
       "      <td>38.827267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.544932</td>\n",
       "      <td>35.744747</td>\n",
       "      <td>29.221090</td>\n",
       "      <td>39.492989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.670540</td>\n",
       "      <td>34.944408</td>\n",
       "      <td>29.231306</td>\n",
       "      <td>38.960350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.278160</td>\n",
       "      <td>38.061348</td>\n",
       "      <td>29.266139</td>\n",
       "      <td>39.057545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.941041</td>\n",
       "      <td>33.870445</td>\n",
       "      <td>29.317945</td>\n",
       "      <td>38.771042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.526881</td>\n",
       "      <td>38.593979</td>\n",
       "      <td>29.433251</td>\n",
       "      <td>39.167721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.679463</td>\n",
       "      <td>36.208527</td>\n",
       "      <td>29.433804</td>\n",
       "      <td>39.021648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.645484</td>\n",
       "      <td>36.554550</td>\n",
       "      <td>29.606642</td>\n",
       "      <td>39.138737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.701516</td>\n",
       "      <td>36.226982</td>\n",
       "      <td>29.787175</td>\n",
       "      <td>39.912777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.3</td>\n",
       "      <td>24.981158</td>\n",
       "      <td>34.084888</td>\n",
       "      <td>29.905944</td>\n",
       "      <td>39.676926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.3</td>\n",
       "      <td>26.199037</td>\n",
       "      <td>35.463985</td>\n",
       "      <td>30.010097</td>\n",
       "      <td>40.023598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.343592</td>\n",
       "      <td>35.920929</td>\n",
       "      <td>30.053556</td>\n",
       "      <td>40.262657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.528030</td>\n",
       "      <td>38.126705</td>\n",
       "      <td>30.108826</td>\n",
       "      <td>40.490662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>26.884275</td>\n",
       "      <td>36.419071</td>\n",
       "      <td>30.195914</td>\n",
       "      <td>40.056438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.6</td>\n",
       "      <td>27.099247</td>\n",
       "      <td>36.687042</td>\n",
       "      <td>30.426871</td>\n",
       "      <td>40.424736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.990224</td>\n",
       "      <td>35.174038</td>\n",
       "      <td>30.451140</td>\n",
       "      <td>40.403301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.8</td>\n",
       "      <td>28.767293</td>\n",
       "      <td>38.618267</td>\n",
       "      <td>30.591120</td>\n",
       "      <td>41.135548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.547788</td>\n",
       "      <td>36.280121</td>\n",
       "      <td>30.907991</td>\n",
       "      <td>40.832619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.3</td>\n",
       "      <td>26.826512</td>\n",
       "      <td>36.393665</td>\n",
       "      <td>30.916367</td>\n",
       "      <td>40.932510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.8</td>\n",
       "      <td>29.572557</td>\n",
       "      <td>39.916634</td>\n",
       "      <td>31.164807</td>\n",
       "      <td>41.553349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24.874289</td>\n",
       "      <td>33.636223</td>\n",
       "      <td>31.168064</td>\n",
       "      <td>41.399044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>27.690697</td>\n",
       "      <td>37.430817</td>\n",
       "      <td>31.387508</td>\n",
       "      <td>41.407379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.206889</td>\n",
       "      <td>37.936802</td>\n",
       "      <td>31.474307</td>\n",
       "      <td>42.239124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>30.380223</td>\n",
       "      <td>40.728981</td>\n",
       "      <td>32.468943</td>\n",
       "      <td>43.798622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.948948</td>\n",
       "      <td>40.094315</td>\n",
       "      <td>32.725256</td>\n",
       "      <td>43.458214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.164450</td>\n",
       "      <td>35.191757</td>\n",
       "      <td>33.457877</td>\n",
       "      <td>43.411068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.6</td>\n",
       "      <td>28.054903</td>\n",
       "      <td>37.949417</td>\n",
       "      <td>33.603666</td>\n",
       "      <td>44.456718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  layer_count  node_number  dropout       loss       rmse  \\\n",
       "0       25            3          160      0.3  24.845056  33.734009   \n",
       "1       25            2          160      0.3  25.494295  34.789776   \n",
       "2       25            3          180      0.7  26.824999  36.214920   \n",
       "3       25            2          180      0.8  28.288883  38.149914   \n",
       "4       25            2          180      0.7  27.182920  36.467155   \n",
       "5       25            3          160      0.7  26.703194  36.160015   \n",
       "6       25            2          160      0.7  26.116151  35.460846   \n",
       "7       25            2          180      0.5  25.228306  34.108627   \n",
       "8       25            2          100      0.3  26.567664  35.971237   \n",
       "9       25            2          160      0.8  28.034410  37.920330   \n",
       "10      25            2          140      0.7  26.723766  36.383057   \n",
       "11      25            3          140      0.3  25.362162  34.922558   \n",
       "12      25            3          140      0.6  29.156775  38.987270   \n",
       "13      25            3          100      0.7  28.347189  38.367676   \n",
       "14      25            3          140      0.7  28.720046  38.750923   \n",
       "15      25            3          120      0.5  25.906912  35.073952   \n",
       "16      25            2          120      0.5  27.278567  36.986805   \n",
       "17      25            3          100      0.6  28.427894  38.370934   \n",
       "18      25            3          120      0.7  28.032149  37.744232   \n",
       "19      25            3          160      0.6  26.197132  35.700542   \n",
       "20      25            2          100      0.6  26.880777  36.623810   \n",
       "21      25            3          180      0.8  27.524914  37.240166   \n",
       "22      25            3          160      0.8  27.277222  36.800636   \n",
       "23      25            3          140      0.8  28.685619  38.522980   \n",
       "24      25            2          140      0.5  26.544932  35.744747   \n",
       "25      25            3          160      0.5  25.670540  34.944408   \n",
       "26      25            2          120      0.7  28.278160  38.061348   \n",
       "27      25            2          180      0.3  24.941041  33.870445   \n",
       "28      25            2          100      0.7  28.526881  38.593979   \n",
       "29      25            3          120      0.6  26.679463  36.208527   \n",
       "30      25            3          180      0.6  26.645484  36.554550   \n",
       "31      25            2          180      0.6  26.701516  36.226982   \n",
       "32      25            3          180      0.3  24.981158  34.084888   \n",
       "33      25            3          120      0.3  26.199037  35.463985   \n",
       "34      25            3          140      0.5  26.343592  35.920929   \n",
       "35      25            2          120      0.8  28.528030  38.126705   \n",
       "36      25            3          100      0.8  26.884275  36.419071   \n",
       "37      25            2          140      0.6  27.099247  36.687042   \n",
       "38      25            2          140      0.3  25.990224  35.174038   \n",
       "39      25            2          140      0.8  28.767293  38.618267   \n",
       "40      25            2          160      0.6  26.547788  36.280121   \n",
       "41      25            2          120      0.3  26.826512  36.393665   \n",
       "42      25            3          120      0.8  29.572557  39.916634   \n",
       "43      25            3          180      0.5  24.874289  33.636223   \n",
       "44      25            3          100      0.3  27.690697  37.430817   \n",
       "45      25            2          100      0.5  28.206889  37.936802   \n",
       "46      25            2          100      0.8  30.380223  40.728981   \n",
       "47      25            3          100      0.5  29.948948  40.094315   \n",
       "48      25            2          160      0.5  26.164450  35.191757   \n",
       "49      25            2          120      0.6  28.054903  37.949417   \n",
       "\n",
       "     val_loss   val_rmse  \n",
       "0   26.676975  36.386173  \n",
       "1   26.759470  36.620213  \n",
       "2   26.982515  36.409119  \n",
       "3   27.069027  36.188385  \n",
       "4   27.242572  36.849945  \n",
       "5   27.409861  37.074490  \n",
       "6   27.493419  36.974094  \n",
       "7   27.522308  37.175056  \n",
       "8   27.772507  37.656372  \n",
       "9   28.028752  37.688915  \n",
       "10  28.099579  37.855316  \n",
       "11  28.102494  37.789055  \n",
       "12  28.160024  37.840137  \n",
       "13  28.277698  38.089466  \n",
       "14  28.379128  38.918564  \n",
       "15  28.398543  38.345280  \n",
       "16  28.417298  37.878738  \n",
       "17  28.488486  38.448299  \n",
       "18  28.556961  38.112572  \n",
       "19  28.750953  39.011215  \n",
       "20  28.934041  38.583847  \n",
       "21  29.075184  38.476078  \n",
       "22  29.125298  39.430817  \n",
       "23  29.193312  38.827267  \n",
       "24  29.221090  39.492989  \n",
       "25  29.231306  38.960350  \n",
       "26  29.266139  39.057545  \n",
       "27  29.317945  38.771042  \n",
       "28  29.433251  39.167721  \n",
       "29  29.433804  39.021648  \n",
       "30  29.606642  39.138737  \n",
       "31  29.787175  39.912777  \n",
       "32  29.905944  39.676926  \n",
       "33  30.010097  40.023598  \n",
       "34  30.053556  40.262657  \n",
       "35  30.108826  40.490662  \n",
       "36  30.195914  40.056438  \n",
       "37  30.426871  40.424736  \n",
       "38  30.451140  40.403301  \n",
       "39  30.591120  41.135548  \n",
       "40  30.907991  40.832619  \n",
       "41  30.916367  40.932510  \n",
       "42  31.164807  41.553349  \n",
       "43  31.168064  41.399044  \n",
       "44  31.387508  41.407379  \n",
       "45  31.474307  42.239124  \n",
       "46  32.468943  43.798622  \n",
       "47  32.725256  43.458214  \n",
       "48  33.457877  43.411068  \n",
       "49  33.603666  44.456718  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.evaluations.to_csv(f'results\\\\GridSearch_Results\\\\unic2g_grid_search_{datetime.now().strftime(\"%m-%d-%Y_%Hh%Mmin%Ss\")}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47c0351e4694c4d80d5f5328433b052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 332.6501 - rmse: 424.0037 - val_loss: 81.8265 - val_rmse: 100.3055\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 72.2040 - rmse: 89.4203 - val_loss: 90.0662 - val_rmse: 110.4281\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 65.8759 - rmse: 82.4931 - val_loss: 71.3898 - val_rmse: 89.5847\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 55.8453 - rmse: 71.8360 - val_loss: 51.2950 - val_rmse: 68.0891\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 48.0122 - rmse: 62.3496 - val_loss: 59.1287 - val_rmse: 78.4267\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 5s 106ms/step - loss: 43.6423 - rmse: 57.1531 - val_loss: 45.0024 - val_rmse: 59.2179\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 41.3557 - rmse: 54.1416 - val_loss: 47.5573 - val_rmse: 62.5104\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 39.0038 - rmse: 51.1294 - val_loss: 36.6757 - val_rmse: 47.7709\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 3s 66ms/step - loss: 37.1637 - rmse: 49.0402 - val_loss: 35.3620 - val_rmse: 47.4451\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 4s 71ms/step - loss: 36.1691 - rmse: 47.9256 - val_loss: 31.9967 - val_rmse: 42.7847\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 3s 65ms/step - loss: 32.9564 - rmse: 43.7759 - val_loss: 32.4073 - val_rmse: 43.6525\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 3s 68ms/step - loss: 33.5926 - rmse: 44.7178 - val_loss: 32.7309 - val_rmse: 42.3267\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 31.6516 - rmse: 41.8487 - val_loss: 32.6251 - val_rmse: 44.2554\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 30.0450 - rmse: 39.8571 - val_loss: 33.1750 - val_rmse: 43.0277\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 3s 65ms/step - loss: 29.1640 - rmse: 38.6092 - val_loss: 33.4520 - val_rmse: 43.0301\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 27.4192 - rmse: 36.5536 - val_loss: 44.2405 - val_rmse: 58.1567\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 3s 69ms/step - loss: 26.5835 - rmse: 35.4434 - val_loss: 39.3233 - val_rmse: 50.4549\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 3s 64ms/step - loss: 25.5022 - rmse: 34.0146 - val_loss: 37.1852 - val_rmse: 49.3603\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 25.2761 - rmse: 33.6622 - val_loss: 31.5939 - val_rmse: 42.2087\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 24.6296 - rmse: 32.9066 - val_loss: 27.8902 - val_rmse: 37.8546\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 23.5947 - rmse: 31.4562 - val_loss: 37.3792 - val_rmse: 48.4183\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 4s 70ms/step - loss: 22.6367 - rmse: 30.4597 - val_loss: 31.1167 - val_rmse: 41.2644\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 22.9245 - rmse: 30.6292 - val_loss: 37.8736 - val_rmse: 48.7579\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 3s 64ms/step - loss: 22.0526 - rmse: 29.4390 - val_loss: 25.1643 - val_rmse: 33.8056\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 21.8082 - rmse: 29.0768 - val_loss: 27.0500 - val_rmse: 36.8508\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 166.6694 - rmse: 198.0025 - val_loss: 102.7130 - val_rmse: 124.6768\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 71.7837 - rmse: 89.0175 - val_loss: 92.8600 - val_rmse: 115.7069\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 59.3208 - rmse: 75.8619 - val_loss: 49.1795 - val_rmse: 66.9148\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 48.8400 - rmse: 63.6633 - val_loss: 45.2808 - val_rmse: 58.7111\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 44.4807 - rmse: 58.4676 - val_loss: 40.3890 - val_rmse: 53.0618\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 3s 66ms/step - loss: 40.9930 - rmse: 53.8561 - val_loss: 39.5258 - val_rmse: 50.7787\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 3s 59ms/step - loss: 38.1017 - rmse: 50.2610 - val_loss: 37.6317 - val_rmse: 47.3956\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 36.1814 - rmse: 47.6136 - val_loss: 36.4284 - val_rmse: 47.4276\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 34.8254 - rmse: 45.9639 - val_loss: 32.5603 - val_rmse: 43.4544\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 33.2808 - rmse: 43.9381 - val_loss: 34.1388 - val_rmse: 46.6067\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 3s 67ms/step - loss: 31.6582 - rmse: 41.9365 - val_loss: 31.0761 - val_rmse: 42.1889\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 31.6994 - rmse: 41.8666 - val_loss: 28.0368 - val_rmse: 38.0622\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 29.2533 - rmse: 38.7022 - val_loss: 38.6063 - val_rmse: 51.7210\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 28.9402 - rmse: 38.3532 - val_loss: 29.2850 - val_rmse: 39.7306\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 27.9543 - rmse: 37.0352 - val_loss: 30.3104 - val_rmse: 40.0098\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 3s 65ms/step - loss: 26.9457 - rmse: 35.6875 - val_loss: 31.3116 - val_rmse: 41.5650\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 25.8731 - rmse: 34.2731 - val_loss: 33.0882 - val_rmse: 43.6608\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 25.0919 - rmse: 33.3962 - val_loss: 32.4774 - val_rmse: 42.8039\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 25.0721 - rmse: 33.0203 - val_loss: 35.9099 - val_rmse: 47.5469\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 24.4219 - rmse: 32.5120 - val_loss: 28.1449 - val_rmse: 38.3834\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 3s 66ms/step - loss: 24.0384 - rmse: 31.7917 - val_loss: 29.3331 - val_rmse: 39.6084\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 3s 64ms/step - loss: 23.7563 - rmse: 31.4242 - val_loss: 28.6285 - val_rmse: 39.4413\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 23.1823 - rmse: 30.9118 - val_loss: 25.4264 - val_rmse: 35.1727\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 3s 60ms/step - loss: 22.1500 - rmse: 29.6194 - val_loss: 27.4142 - val_rmse: 37.5077\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 3s 61ms/step - loss: 22.6666 - rmse: 30.1885 - val_loss: 25.6807 - val_rmse: 35.3012\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 8s 155ms/step - loss: 112.2257 - rmse: 136.8674 - val_loss: 61.9155 - val_rmse: 76.7966\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 71.3070 - rmse: 88.0488 - val_loss: 62.4528 - val_rmse: 76.1548\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 62.5209 - rmse: 78.3339 - val_loss: 69.2127 - val_rmse: 90.0592\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 51.1261 - rmse: 65.2591 - val_loss: 43.8743 - val_rmse: 56.0405\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 45.2626 - rmse: 58.3807 - val_loss: 50.9789 - val_rmse: 66.9761\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 42.7777 - rmse: 55.6397 - val_loss: 42.1085 - val_rmse: 56.5567\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 38.6727 - rmse: 50.8984 - val_loss: 42.2612 - val_rmse: 56.0457\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 36.5054 - rmse: 48.3146 - val_loss: 34.3715 - val_rmse: 46.2105\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 35.4712 - rmse: 46.8916 - val_loss: 34.3794 - val_rmse: 46.6000\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 33.3855 - rmse: 44.1903 - val_loss: 31.2826 - val_rmse: 42.2441\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 31.9634 - rmse: 42.2210 - val_loss: 33.3821 - val_rmse: 45.1636\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 28.9332 - rmse: 38.3222 - val_loss: 29.8265 - val_rmse: 40.0354\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 29.3913 - rmse: 38.7241 - val_loss: 32.3765 - val_rmse: 44.3903\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 27.1603 - rmse: 36.0764 - val_loss: 33.1694 - val_rmse: 44.1467\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 4s 76ms/step - loss: 25.6813 - rmse: 34.0156 - val_loss: 34.3177 - val_rmse: 44.5265\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 25.3837 - rmse: 33.7088 - val_loss: 32.7760 - val_rmse: 42.5624\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 24.4141 - rmse: 32.3471 - val_loss: 37.1833 - val_rmse: 48.6936\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 23.9712 - rmse: 31.8312 - val_loss: 31.8220 - val_rmse: 43.3200\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 23.4379 - rmse: 31.0809 - val_loss: 35.1815 - val_rmse: 47.0113\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 22.5089 - rmse: 29.9203 - val_loss: 28.3760 - val_rmse: 38.4560\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 22.1096 - rmse: 29.4758 - val_loss: 27.5211 - val_rmse: 36.3428\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 22.1983 - rmse: 29.5027 - val_loss: 26.3937 - val_rmse: 35.5659\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 21.5610 - rmse: 28.6777 - val_loss: 32.2283 - val_rmse: 42.5979\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 20.6934 - rmse: 27.5921 - val_loss: 26.2728 - val_rmse: 35.4927\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 20.8162 - rmse: 27.8242 - val_loss: 27.2051 - val_rmse: 36.9723\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "50/50 [==============================] - 8s 153ms/step - loss: 87.1800 - rmse: 106.6353 - val_loss: 78.9819 - val_rmse: 96.9826\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 67.7589 - rmse: 84.0331 - val_loss: 59.6272 - val_rmse: 76.0209\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 51.6570 - rmse: 65.9329 - val_loss: 50.2838 - val_rmse: 65.9103\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 46.0533 - rmse: 60.2561 - val_loss: 43.0861 - val_rmse: 54.2504\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 42.2817 - rmse: 55.3963 - val_loss: 36.2484 - val_rmse: 49.2830\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 39.4755 - rmse: 51.9950 - val_loss: 34.4959 - val_rmse: 45.4316\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 37.7679 - rmse: 49.7904 - val_loss: 37.4425 - val_rmse: 47.2785\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 35.2482 - rmse: 46.8085 - val_loss: 31.1760 - val_rmse: 41.7525\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 33.2924 - rmse: 44.1801 - val_loss: 48.8738 - val_rmse: 62.6543\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 32.5963 - rmse: 43.1087 - val_loss: 30.2061 - val_rmse: 40.6323\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 31.2925 - rmse: 41.7038 - val_loss: 34.2339 - val_rmse: 46.1020\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 29.4863 - rmse: 39.1526 - val_loss: 28.5096 - val_rmse: 38.6157\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 29.1794 - rmse: 38.6152 - val_loss: 32.9092 - val_rmse: 44.4261\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 27.8402 - rmse: 36.5285 - val_loss: 28.3662 - val_rmse: 37.8181\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 26.6882 - rmse: 35.4138 - val_loss: 27.9141 - val_rmse: 37.3533\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 26.3879 - rmse: 35.0538 - val_loss: 32.1785 - val_rmse: 42.5473\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 24.7507 - rmse: 32.9990 - val_loss: 27.0341 - val_rmse: 37.3193\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 24.8820 - rmse: 33.1359 - val_loss: 27.9045 - val_rmse: 38.1412\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 24.6050 - rmse: 32.9239 - val_loss: 34.3932 - val_rmse: 45.5355\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 24.1964 - rmse: 32.1540 - val_loss: 33.1234 - val_rmse: 43.4834\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 23.5963 - rmse: 31.3618 - val_loss: 29.5190 - val_rmse: 39.5011\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 22.6470 - rmse: 30.1007 - val_loss: 32.1327 - val_rmse: 43.0475\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 22.8266 - rmse: 30.3535 - val_loss: 26.8393 - val_rmse: 37.2026\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 22.1087 - rmse: 29.5745 - val_loss: 26.9092 - val_rmse: 36.8558\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 21.7541 - rmse: 29.0091 - val_loss: 25.6320 - val_rmse: 35.5922\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 120.8383 - rmse: 145.9949 - val_loss: 90.9087 - val_rmse: 110.5198\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 71.9650 - rmse: 88.7866 - val_loss: 84.6935 - val_rmse: 104.1744\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 57.1979 - rmse: 72.5804 - val_loss: 75.2243 - val_rmse: 90.6787\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 48.0848 - rmse: 62.2191 - val_loss: 39.7156 - val_rmse: 50.4773\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 43.5661 - rmse: 56.7889 - val_loss: 37.6645 - val_rmse: 48.9821\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 39.6253 - rmse: 51.8574 - val_loss: 47.2191 - val_rmse: 61.8964\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 36.8591 - rmse: 48.3330 - val_loss: 35.4921 - val_rmse: 46.9838\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 35.5631 - rmse: 46.6403 - val_loss: 34.2192 - val_rmse: 44.3939\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 33.7111 - rmse: 44.5283 - val_loss: 34.9892 - val_rmse: 46.5876\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 31.4176 - rmse: 41.7711 - val_loss: 30.4421 - val_rmse: 40.8916\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 31.1183 - rmse: 41.0131 - val_loss: 35.5274 - val_rmse: 47.4615\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 29.2206 - rmse: 38.6668 - val_loss: 29.9827 - val_rmse: 39.0350\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 28.0469 - rmse: 37.1475 - val_loss: 29.7144 - val_rmse: 40.0253\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 27.0522 - rmse: 35.8611 - val_loss: 30.1187 - val_rmse: 40.1242\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 26.3858 - rmse: 34.9430 - val_loss: 27.6150 - val_rmse: 37.4047\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 24.9301 - rmse: 33.1693 - val_loss: 30.1128 - val_rmse: 40.9263\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 24.9215 - rmse: 33.0951 - val_loss: 48.0500 - val_rmse: 63.0325\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 23.9670 - rmse: 32.0937 - val_loss: 34.1254 - val_rmse: 46.5721\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 23.6109 - rmse: 31.4510 - val_loss: 38.8013 - val_rmse: 51.0899\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 22.9030 - rmse: 30.6306 - val_loss: 30.9914 - val_rmse: 41.7301\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 22.2755 - rmse: 29.7782 - val_loss: 33.6420 - val_rmse: 44.6067\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 21.5124 - rmse: 28.7533 - val_loss: 27.1179 - val_rmse: 36.3501\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 21.3347 - rmse: 28.5249 - val_loss: 25.5062 - val_rmse: 34.4077\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 21.2557 - rmse: 28.2538 - val_loss: 26.3809 - val_rmse: 35.4662\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 20.4376 - rmse: 27.3773 - val_loss: 24.7796 - val_rmse: 33.9053\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 219.6951 - rmse: 257.3715 - val_loss: 66.2220 - val_rmse: 82.8164\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 76.8083 - rmse: 95.1865 - val_loss: 84.7054 - val_rmse: 103.9306\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 71.2733 - rmse: 88.9072 - val_loss: 100.1562 - val_rmse: 120.7717\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 54.4331 - rmse: 69.7795 - val_loss: 54.5261 - val_rmse: 66.1643\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 46.5566 - rmse: 60.7392 - val_loss: 52.6744 - val_rmse: 68.7490\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 42.6516 - rmse: 55.9920 - val_loss: 37.2037 - val_rmse: 49.9892\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 40.0763 - rmse: 52.9532 - val_loss: 39.3016 - val_rmse: 51.1989\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 37.1727 - rmse: 48.8603 - val_loss: 39.7263 - val_rmse: 50.4477\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 35.5703 - rmse: 46.8152 - val_loss: 34.2518 - val_rmse: 46.6519\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 34.0440 - rmse: 45.1923 - val_loss: 31.9653 - val_rmse: 43.1134\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 32.0194 - rmse: 42.4595 - val_loss: 34.4875 - val_rmse: 47.0069\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 31.9545 - rmse: 42.0848 - val_loss: 31.9603 - val_rmse: 42.1999\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 30.1034 - rmse: 39.8869 - val_loss: 38.6033 - val_rmse: 51.8556\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 28.9667 - rmse: 38.3591 - val_loss: 32.2546 - val_rmse: 43.0370\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 27.3262 - rmse: 36.2375 - val_loss: 33.7703 - val_rmse: 44.0737\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 26.7949 - rmse: 35.4949 - val_loss: 29.8774 - val_rmse: 39.1058\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 26.5711 - rmse: 35.2316 - val_loss: 46.8664 - val_rmse: 59.6990\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 25.4145 - rmse: 33.6668 - val_loss: 40.1157 - val_rmse: 52.7439\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 24.5640 - rmse: 32.7058 - val_loss: 36.0021 - val_rmse: 48.4958\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 24.2036 - rmse: 32.0979 - val_loss: 34.2486 - val_rmse: 45.6329\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 23.2212 - rmse: 30.9673 - val_loss: 30.7705 - val_rmse: 40.6264\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 23.6532 - rmse: 31.2392 - val_loss: 26.3878 - val_rmse: 35.8853\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 22.3429 - rmse: 29.6539 - val_loss: 27.2896 - val_rmse: 36.8882\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 22.3224 - rmse: 29.7484 - val_loss: 28.6057 - val_rmse: 39.0251\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 21.5443 - rmse: 28.8208 - val_loss: 28.0508 - val_rmse: 38.8746\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 150.9649 - rmse: 181.8980 - val_loss: 78.8385 - val_rmse: 96.8563\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 6s 114ms/step - loss: 73.2945 - rmse: 90.4943 - val_loss: 71.8997 - val_rmse: 89.3829\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 6s 117ms/step - loss: 58.2516 - rmse: 73.9197 - val_loss: 46.6403 - val_rmse: 62.5681\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 6s 110ms/step - loss: 46.4904 - rmse: 60.4184 - val_loss: 59.1725 - val_rmse: 71.9014\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 5s 110ms/step - loss: 44.1838 - rmse: 57.3687 - val_loss: 39.1450 - val_rmse: 51.4803\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 40.4092 - rmse: 52.9033 - val_loss: 38.0141 - val_rmse: 49.1828\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 5s 110ms/step - loss: 38.1026 - rmse: 50.1703 - val_loss: 43.0542 - val_rmse: 56.7444\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 6s 112ms/step - loss: 36.0660 - rmse: 47.6002 - val_loss: 34.2354 - val_rmse: 44.7969\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 34.4459 - rmse: 45.6636 - val_loss: 33.3929 - val_rmse: 44.9601\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 5s 109ms/step - loss: 32.8314 - rmse: 43.4311 - val_loss: 31.3377 - val_rmse: 41.8640\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 31.2838 - rmse: 41.1784 - val_loss: 43.6465 - val_rmse: 57.0136\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 29.9562 - rmse: 39.6600 - val_loss: 29.7097 - val_rmse: 39.9736\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 5s 109ms/step - loss: 28.3368 - rmse: 37.3076 - val_loss: 38.6339 - val_rmse: 51.5533\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 6s 114ms/step - loss: 27.6137 - rmse: 36.3443 - val_loss: 31.2309 - val_rmse: 41.7966\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 26.4827 - rmse: 35.1304 - val_loss: 26.9624 - val_rmse: 36.6403\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 5s 108ms/step - loss: 25.4159 - rmse: 33.5368 - val_loss: 34.3840 - val_rmse: 47.1425\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 6s 113ms/step - loss: 24.6716 - rmse: 32.6911 - val_loss: 29.5044 - val_rmse: 39.7278\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 6s 126ms/step - loss: 23.9937 - rmse: 31.9408 - val_loss: 31.4028 - val_rmse: 42.3845\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 5s 109ms/step - loss: 22.9223 - rmse: 30.7248 - val_loss: 42.2789 - val_rmse: 56.1065\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 23.1781 - rmse: 30.8854 - val_loss: 36.0738 - val_rmse: 47.7877\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 5s 109ms/step - loss: 22.4476 - rmse: 29.8993 - val_loss: 37.4981 - val_rmse: 49.1972\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 21.7127 - rmse: 28.8739 - val_loss: 26.7639 - val_rmse: 35.9814\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 21.4194 - rmse: 28.5603 - val_loss: 34.6580 - val_rmse: 45.8267\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 5s 109ms/step - loss: 21.0059 - rmse: 27.9828 - val_loss: 30.6157 - val_rmse: 41.1643\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 20.5413 - rmse: 27.3505 - val_loss: 25.5708 - val_rmse: 34.4081\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 96.8548 - rmse: 119.8239 - val_loss: 92.2442 - val_rmse: 112.5259\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 6s 123ms/step - loss: 70.9783 - rmse: 88.1273 - val_loss: 51.1834 - val_rmse: 63.3458\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 6s 122ms/step - loss: 57.1850 - rmse: 72.9474 - val_loss: 44.4269 - val_rmse: 58.3777\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 47.8126 - rmse: 62.3921 - val_loss: 41.7131 - val_rmse: 52.2515\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 6s 118ms/step - loss: 43.5821 - rmse: 57.0666 - val_loss: 45.5542 - val_rmse: 61.3260\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 6s 117ms/step - loss: 39.5497 - rmse: 52.3398 - val_loss: 45.8743 - val_rmse: 56.3659\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 6s 117ms/step - loss: 37.7795 - rmse: 49.7935 - val_loss: 42.0136 - val_rmse: 55.3071\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 6s 122ms/step - loss: 36.7140 - rmse: 48.4278 - val_loss: 32.8653 - val_rmse: 43.8290\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 35.0585 - rmse: 46.4360 - val_loss: 37.4718 - val_rmse: 50.4290\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 6s 118ms/step - loss: 33.0793 - rmse: 43.9396 - val_loss: 32.0172 - val_rmse: 43.0077\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 6s 120ms/step - loss: 32.0738 - rmse: 42.5116 - val_loss: 42.5252 - val_rmse: 55.9445\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 6s 118ms/step - loss: 31.1178 - rmse: 41.0826 - val_loss: 28.5585 - val_rmse: 38.7910\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 28.8878 - rmse: 38.1892 - val_loss: 30.6447 - val_rmse: 41.5411\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 6s 120ms/step - loss: 28.7272 - rmse: 38.0145 - val_loss: 28.3380 - val_rmse: 37.1733\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 27.4177 - rmse: 36.2146 - val_loss: 28.0962 - val_rmse: 37.2571\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 6s 120ms/step - loss: 26.2457 - rmse: 34.7871 - val_loss: 27.2888 - val_rmse: 36.3653\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 6s 117ms/step - loss: 25.8770 - rmse: 34.3300 - val_loss: 33.1919 - val_rmse: 42.8808\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 6s 118ms/step - loss: 25.3243 - rmse: 33.7538 - val_loss: 32.7609 - val_rmse: 44.2701\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 6s 121ms/step - loss: 24.4631 - rmse: 32.5061 - val_loss: 26.9517 - val_rmse: 36.1573\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 6s 118ms/step - loss: 23.5216 - rmse: 31.4158 - val_loss: 26.4262 - val_rmse: 35.8174\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 23.0884 - rmse: 30.5403 - val_loss: 28.9196 - val_rmse: 39.2872\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 6s 121ms/step - loss: 22.7201 - rmse: 30.1524 - val_loss: 28.4396 - val_rmse: 39.1978\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 22.3234 - rmse: 29.8469 - val_loss: 27.1640 - val_rmse: 36.3070\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 6s 120ms/step - loss: 22.2833 - rmse: 29.4820 - val_loss: 28.0682 - val_rmse: 38.8589\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 6s 119ms/step - loss: 21.9484 - rmse: 29.1305 - val_loss: 24.8614 - val_rmse: 34.6810\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 10s 206ms/step - loss: 3166.8193 - rmse: 3609.4832 - val_loss: 1355.3103 - val_rmse: 1813.8755\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 189.9937 - rmse: 256.9230 - val_loss: 68.4685 - val_rmse: 83.0053\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 71.0615 - rmse: 87.1082 - val_loss: 69.6010 - val_rmse: 85.4332\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 8s 154ms/step - loss: 75.1075 - rmse: 93.2090 - val_loss: 68.0600 - val_rmse: 86.6003\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 56.7919 - rmse: 72.4595 - val_loss: 76.6083 - val_rmse: 93.3500\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 142ms/step - loss: 49.1768 - rmse: 64.0467 - val_loss: 43.0825 - val_rmse: 56.6317\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 43.5339 - rmse: 57.0948 - val_loss: 43.2283 - val_rmse: 57.3911\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 142ms/step - loss: 40.4708 - rmse: 53.2944 - val_loss: 39.6097 - val_rmse: 50.7671\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 39.2294 - rmse: 51.7666 - val_loss: 33.4651 - val_rmse: 43.9710\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 36.2974 - rmse: 48.0024 - val_loss: 33.2731 - val_rmse: 44.8780\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 34.2189 - rmse: 45.3383 - val_loss: 46.3128 - val_rmse: 60.4689\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 32.7321 - rmse: 43.5024 - val_loss: 32.1390 - val_rmse: 43.3143\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 7s 145ms/step - loss: 31.4968 - rmse: 42.0694 - val_loss: 32.1336 - val_rmse: 43.3317\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 29.8055 - rmse: 39.6221 - val_loss: 32.6216 - val_rmse: 42.4295\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 28.5454 - rmse: 38.0122 - val_loss: 27.9039 - val_rmse: 37.1515\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 27.3086 - rmse: 36.6664 - val_loss: 28.9218 - val_rmse: 38.9153\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 25.9942 - rmse: 34.9223 - val_loss: 33.3711 - val_rmse: 43.4559\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 25.7076 - rmse: 34.3006 - val_loss: 41.2377 - val_rmse: 53.6500\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 24.8120 - rmse: 33.0529 - val_loss: 38.2009 - val_rmse: 49.8608\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 23.9648 - rmse: 32.0103 - val_loss: 39.9027 - val_rmse: 52.0968\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 23.2026 - rmse: 30.9908 - val_loss: 37.7043 - val_rmse: 50.4667\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 8s 151ms/step - loss: 22.7854 - rmse: 30.3633 - val_loss: 30.6336 - val_rmse: 41.1229\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 145ms/step - loss: 22.0000 - rmse: 29.3834 - val_loss: 35.3872 - val_rmse: 46.5388\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 8s 158ms/step - loss: 21.8781 - rmse: 29.2270 - val_loss: 26.4454 - val_rmse: 36.0434\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 20.8843 - rmse: 27.9563 - val_loss: 27.3488 - val_rmse: 37.8334\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 10s 200ms/step - loss: 5728.0024 - rmse: 6735.4683 - val_loss: 98.1195 - val_rmse: 120.8671\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 77.3949 - rmse: 95.1560 - val_loss: 61.8138 - val_rmse: 75.2678\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 139ms/step - loss: 69.6527 - rmse: 85.5975 - val_loss: 88.1979 - val_rmse: 108.0785\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 66.4698 - rmse: 82.9410 - val_loss: 51.6049 - val_rmse: 65.0717\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 53.7934 - rmse: 69.4860 - val_loss: 53.9622 - val_rmse: 70.0034\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 47.5455 - rmse: 61.5753 - val_loss: 47.9463 - val_rmse: 58.7383\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 138ms/step - loss: 42.7171 - rmse: 55.7846 - val_loss: 49.4566 - val_rmse: 64.8264\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 40.4378 - rmse: 52.9599 - val_loss: 37.7835 - val_rmse: 48.3145\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 37.7994 - rmse: 49.4027 - val_loss: 37.2138 - val_rmse: 49.9590\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 35.2602 - rmse: 46.7490 - val_loss: 32.7408 - val_rmse: 44.2060\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 33.5891 - rmse: 44.6001 - val_loss: 38.1203 - val_rmse: 51.0875\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 31.5226 - rmse: 41.8199 - val_loss: 30.8046 - val_rmse: 40.2710\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 31.9132 - rmse: 42.0723 - val_loss: 33.1295 - val_rmse: 44.6008\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 30.7742 - rmse: 40.4689 - val_loss: 28.4847 - val_rmse: 37.2693\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 29.5285 - rmse: 38.8200 - val_loss: 31.0429 - val_rmse: 41.5723\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 27.9763 - rmse: 37.0434 - val_loss: 39.3930 - val_rmse: 50.6021\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 27.4141 - rmse: 36.1969 - val_loss: 36.9527 - val_rmse: 47.3147\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 26.1618 - rmse: 34.7877 - val_loss: 40.2213 - val_rmse: 52.2935\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 25.9270 - rmse: 34.4006 - val_loss: 33.8010 - val_rmse: 44.8958\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 25.2792 - rmse: 33.7800 - val_loss: 27.8575 - val_rmse: 37.0998\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 24.9591 - rmse: 33.1431 - val_loss: 35.2343 - val_rmse: 46.1380\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 24.0199 - rmse: 32.0920 - val_loss: 32.1765 - val_rmse: 42.1721\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 23.3205 - rmse: 31.1906 - val_loss: 27.0104 - val_rmse: 35.9604\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 7s 141ms/step - loss: 23.1397 - rmse: 30.8865 - val_loss: 32.5511 - val_rmse: 42.1833\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 23.3302 - rmse: 31.0356 - val_loss: 29.1585 - val_rmse: 39.6353\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 11s 223ms/step - loss: 4077.4206 - rmse: 4769.6802 - val_loss: 114.4925 - val_rmse: 135.2014\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 8s 153ms/step - loss: 73.9560 - rmse: 90.3090 - val_loss: 90.6876 - val_rmse: 110.2067\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 8s 160ms/step - loss: 70.2483 - rmse: 86.9880 - val_loss: 69.2219 - val_rmse: 82.2216\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 8s 153ms/step - loss: 60.0275 - rmse: 76.3701 - val_loss: 46.5028 - val_rmse: 61.5215\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 51.6608 - rmse: 66.4893 - val_loss: 44.5908 - val_rmse: 55.3178\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 8s 156ms/step - loss: 43.9861 - rmse: 56.7491 - val_loss: 39.5808 - val_rmse: 52.1559\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 8s 159ms/step - loss: 39.6817 - rmse: 51.9384 - val_loss: 36.7547 - val_rmse: 48.4202\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 8s 154ms/step - loss: 36.9560 - rmse: 48.3901 - val_loss: 34.2192 - val_rmse: 45.8787\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 8s 157ms/step - loss: 34.0717 - rmse: 44.7972 - val_loss: 38.0547 - val_rmse: 49.7230\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 8s 153ms/step - loss: 32.4709 - rmse: 42.4164 - val_loss: 46.0772 - val_rmse: 60.5209\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 30.8618 - rmse: 40.4851 - val_loss: 32.0999 - val_rmse: 43.0446\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 28.5429 - rmse: 37.4661 - val_loss: 29.4494 - val_rmse: 40.1137\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 27.1991 - rmse: 35.5897 - val_loss: 31.3219 - val_rmse: 41.7497\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 8s 156ms/step - loss: 26.2684 - rmse: 34.6259 - val_loss: 29.0023 - val_rmse: 38.8921\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 8s 162ms/step - loss: 25.0171 - rmse: 32.9067 - val_loss: 29.4640 - val_rmse: 39.1666\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 23.6298 - rmse: 31.3389 - val_loss: 25.7366 - val_rmse: 34.9562\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 8s 161ms/step - loss: 23.4886 - rmse: 30.9993 - val_loss: 25.6760 - val_rmse: 35.1165\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 22.5019 - rmse: 29.8005 - val_loss: 41.2328 - val_rmse: 53.4872\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 21.6017 - rmse: 28.7271 - val_loss: 33.0654 - val_rmse: 45.7615\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 21.5586 - rmse: 28.4479 - val_loss: 40.1445 - val_rmse: 52.8419\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 173ms/step - loss: 20.4614 - rmse: 27.1835 - val_loss: 27.0938 - val_rmse: 37.2977\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 8s 161ms/step - loss: 19.9168 - rmse: 26.3396 - val_loss: 25.8539 - val_rmse: 35.4534\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 8s 153ms/step - loss: 19.3921 - rmse: 25.7945 - val_loss: 28.0479 - val_rmse: 37.6028\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 8s 158ms/step - loss: 19.7585 - rmse: 26.0780 - val_loss: 24.7673 - val_rmse: 33.1585\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 8s 153ms/step - loss: 18.6777 - rmse: 24.8945 - val_loss: 22.9566 - val_rmse: 31.8184\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 2231.0880 - rmse: 2671.1370 - val_loss: 107.7532 - val_rmse: 128.8413\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 81.6544 - rmse: 99.9403 - val_loss: 61.3786 - val_rmse: 75.0860\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 67.5897 - rmse: 84.8725 - val_loss: 65.8113 - val_rmse: 84.5160\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 59.6098 - rmse: 75.3623 - val_loss: 45.8937 - val_rmse: 57.7425\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 52.6063 - rmse: 67.7386 - val_loss: 50.1252 - val_rmse: 62.9017\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 46.4555 - rmse: 60.2942 - val_loss: 44.4546 - val_rmse: 55.5509\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 42.4567 - rmse: 55.3446 - val_loss: 37.3538 - val_rmse: 49.9185\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 40.0154 - rmse: 52.8958 - val_loss: 34.4817 - val_rmse: 46.4894\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 10s 205ms/step - loss: 37.5408 - rmse: 49.3458 - val_loss: 34.7079 - val_rmse: 46.5947\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 11s 214ms/step - loss: 35.2005 - rmse: 46.6404 - val_loss: 33.5964 - val_rmse: 45.5565\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 11s 213ms/step - loss: 34.1448 - rmse: 44.8271 - val_loss: 36.7879 - val_rmse: 49.7103\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 31.8120 - rmse: 41.9803 - val_loss: 30.7516 - val_rmse: 41.8106\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 31.7622 - rmse: 42.4725 - val_loss: 35.1830 - val_rmse: 47.3269\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 29.5641 - rmse: 39.1372 - val_loss: 28.1948 - val_rmse: 38.1110\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 28.1471 - rmse: 37.3761 - val_loss: 27.3774 - val_rmse: 37.0199\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 28.2772 - rmse: 37.2738 - val_loss: 27.7704 - val_rmse: 37.6146\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 26.2113 - rmse: 34.9578 - val_loss: 34.6801 - val_rmse: 46.0154\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 26.3665 - rmse: 35.0268 - val_loss: 27.8865 - val_rmse: 37.4507\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 24.7374 - rmse: 33.2239 - val_loss: 35.0307 - val_rmse: 46.9983\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 23.8878 - rmse: 31.9816 - val_loss: 35.1509 - val_rmse: 47.3094\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 24.3108 - rmse: 32.4382 - val_loss: 32.2649 - val_rmse: 43.7099\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 22.9513 - rmse: 30.6097 - val_loss: 26.5749 - val_rmse: 35.9489\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 23.0627 - rmse: 30.5485 - val_loss: 29.1012 - val_rmse: 39.5387\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 26.2615 - rmse: 35.9662 - val_loss: 27.4959 - val_rmse: 38.1961\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 21.3109 - rmse: 28.5503 - val_loss: 26.4899 - val_rmse: 35.7480\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 3163.6089 - rmse: 3865.8794 - val_loss: 84.6627 - val_rmse: 102.9746\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 10s 196ms/step - loss: 69.0405 - rmse: 85.6776 - val_loss: 90.9129 - val_rmse: 106.5669\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 186ms/step - loss: 78.3507 - rmse: 97.8901 - val_loss: 74.9720 - val_rmse: 92.6374\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 66.9497 - rmse: 85.2642 - val_loss: 45.2310 - val_rmse: 57.0438\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 10s 194ms/step - loss: 54.2073 - rmse: 69.9397 - val_loss: 67.2737 - val_rmse: 84.5076\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 47.8056 - rmse: 62.1744 - val_loss: 45.5416 - val_rmse: 56.7991\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 41.9354 - rmse: 55.1563 - val_loss: 39.2192 - val_rmse: 49.3498\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 187ms/step - loss: 38.6403 - rmse: 50.8865 - val_loss: 38.2878 - val_rmse: 48.6374\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 36.7759 - rmse: 48.4731 - val_loss: 34.2415 - val_rmse: 45.3127\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 34.1554 - rmse: 45.0668 - val_loss: 32.7666 - val_rmse: 42.9315\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 31.8304 - rmse: 42.1966 - val_loss: 53.5628 - val_rmse: 68.0825\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 10s 196ms/step - loss: 31.1554 - rmse: 41.2124 - val_loss: 29.1547 - val_rmse: 38.8042\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 28.4699 - rmse: 37.9344 - val_loss: 38.5566 - val_rmse: 51.0251\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 10s 192ms/step - loss: 27.7008 - rmse: 36.8543 - val_loss: 36.5511 - val_rmse: 47.4029\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 188ms/step - loss: 26.3658 - rmse: 34.7479 - val_loss: 31.2660 - val_rmse: 41.5457\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 187ms/step - loss: 24.8306 - rmse: 32.9400 - val_loss: 33.4521 - val_rmse: 44.7183\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 24.3258 - rmse: 32.3661 - val_loss: 39.2318 - val_rmse: 51.1460\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 10s 200ms/step - loss: 23.0170 - rmse: 30.7712 - val_loss: 33.9503 - val_rmse: 45.5614\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 11s 210ms/step - loss: 23.2009 - rmse: 30.7608 - val_loss: 34.5929 - val_rmse: 47.2788\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 10s 194ms/step - loss: 21.5511 - rmse: 28.6642 - val_loss: 39.4993 - val_rmse: 51.4683\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 21.6084 - rmse: 28.7163 - val_loss: 36.7234 - val_rmse: 48.6959\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 10s 192ms/step - loss: 19.9526 - rmse: 26.6569 - val_loss: 29.9087 - val_rmse: 40.7071\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 9s 188ms/step - loss: 20.1232 - rmse: 26.9918 - val_loss: 27.4516 - val_rmse: 36.3237\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 19.8937 - rmse: 26.3719 - val_loss: 25.5620 - val_rmse: 34.2003\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 19.1699 - rmse: 25.5466 - val_loss: 27.8413 - val_rmse: 38.3641\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 13s 256ms/step - loss: 7662.5423 - rmse: 8496.6953 - val_loss: 76.4296 - val_rmse: 92.8682\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 73.3139 - rmse: 89.6515 - val_loss: 76.8951 - val_rmse: 94.8175\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 10s 205ms/step - loss: 71.9984 - rmse: 89.1885 - val_loss: 124.9280 - val_rmse: 147.5123\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 69.9020 - rmse: 87.6471 - val_loss: 59.3131 - val_rmse: 72.1566\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 52.7421 - rmse: 68.5757 - val_loss: 57.5291 - val_rmse: 74.6252\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 188ms/step - loss: 45.3784 - rmse: 59.3395 - val_loss: 48.2685 - val_rmse: 58.8367\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 41.1500 - rmse: 53.9857 - val_loss: 34.5983 - val_rmse: 45.8799\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 10s 194ms/step - loss: 38.2317 - rmse: 50.4680 - val_loss: 37.4463 - val_rmse: 48.6270\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 35.3107 - rmse: 46.8025 - val_loss: 39.7206 - val_rmse: 53.0572\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 34.0111 - rmse: 45.1361 - val_loss: 31.6107 - val_rmse: 42.5866\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 186ms/step - loss: 32.3612 - rmse: 43.0021 - val_loss: 29.7571 - val_rmse: 38.9383\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 31.5475 - rmse: 41.7608 - val_loss: 28.8434 - val_rmse: 38.8769\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 10s 190ms/step - loss: 29.4404 - rmse: 39.0805 - val_loss: 29.3352 - val_rmse: 39.7617\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 28.4403 - rmse: 37.5808 - val_loss: 30.5707 - val_rmse: 39.8099\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 27.3698 - rmse: 36.2912 - val_loss: 32.1143 - val_rmse: 42.1503\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 187ms/step - loss: 26.2341 - rmse: 34.8200 - val_loss: 33.7385 - val_rmse: 46.0615\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 25.5497 - rmse: 33.9160 - val_loss: 41.7080 - val_rmse: 54.8123\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 10s 192ms/step - loss: 24.7529 - rmse: 32.8188 - val_loss: 29.0837 - val_rmse: 40.2473\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 23.9844 - rmse: 31.9944 - val_loss: 26.1828 - val_rmse: 35.0962\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 23.7397 - rmse: 31.5156 - val_loss: 26.1488 - val_rmse: 35.1020\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 184ms/step - loss: 23.3269 - rmse: 31.1606 - val_loss: 33.0586 - val_rmse: 44.6790\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 22.6937 - rmse: 30.1717 - val_loss: 30.9631 - val_rmse: 41.6245\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 22.0241 - rmse: 29.4379 - val_loss: 29.7320 - val_rmse: 39.8560\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 21.3788 - rmse: 28.4088 - val_loss: 24.6905 - val_rmse: 33.3297\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 20.9701 - rmse: 27.9367 - val_loss: 24.7434 - val_rmse: 34.5344\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 366.8495 - rmse: 469.2975 - val_loss: 92.5724 - val_rmse: 112.6273\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 83.0542 - rmse: 100.9591 - val_loss: 69.3637 - val_rmse: 84.7331\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 79.1980 - rmse: 97.0798 - val_loss: 73.8218 - val_rmse: 88.1158\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 68.9471 - rmse: 87.3453 - val_loss: 47.8074 - val_rmse: 60.7691\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 52.4235 - rmse: 67.7514 - val_loss: 58.7991 - val_rmse: 72.8480\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 48.2072 - rmse: 62.3159 - val_loss: 41.7683 - val_rmse: 54.0142\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 43.9905 - rmse: 57.3362 - val_loss: 35.5729 - val_rmse: 47.1816\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 39.2760 - rmse: 51.5233 - val_loss: 43.6173 - val_rmse: 54.8640\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 37.6815 - rmse: 49.5017 - val_loss: 32.7130 - val_rmse: 43.1025\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 34.9649 - rmse: 45.4175 - val_loss: 32.2647 - val_rmse: 43.4583\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 33.3397 - rmse: 43.6683 - val_loss: 35.6018 - val_rmse: 47.7425\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 140ms/step - loss: 31.6698 - rmse: 41.4040 - val_loss: 28.3512 - val_rmse: 38.2504\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 30.3617 - rmse: 39.8845 - val_loss: 30.1025 - val_rmse: 40.3124\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 28.5752 - rmse: 37.5028 - val_loss: 28.3338 - val_rmse: 37.4908\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 27.3948 - rmse: 36.2739 - val_loss: 38.4355 - val_rmse: 49.7136\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 26.1476 - rmse: 34.4110 - val_loss: 33.4032 - val_rmse: 44.3312\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 26.1661 - rmse: 34.4771 - val_loss: 35.3653 - val_rmse: 47.2314\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 24.3658 - rmse: 32.0750 - val_loss: 28.7168 - val_rmse: 37.9805\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 23.8088 - rmse: 31.6741 - val_loss: 40.0464 - val_rmse: 52.8620\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 23.2371 - rmse: 30.9199 - val_loss: 34.1230 - val_rmse: 45.2009\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 23.7130 - rmse: 31.3373 - val_loss: 32.7880 - val_rmse: 43.9214\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 7s 131ms/step - loss: 21.6304 - rmse: 28.7827 - val_loss: 26.0508 - val_rmse: 34.9695\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 22.3435 - rmse: 29.6217 - val_loss: 35.7462 - val_rmse: 47.4744\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 21.7068 - rmse: 28.8135 - val_loss: 25.6518 - val_rmse: 34.5011\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 21.2050 - rmse: 28.3084 - val_loss: 26.8732 - val_rmse: 37.1891\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 110.6727 - rmse: 134.1384 - val_loss: 132.8722 - val_rmse: 155.1213\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 81.5835 - rmse: 99.9884 - val_loss: 63.7152 - val_rmse: 80.0402\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 69.7471 - rmse: 86.3621 - val_loss: 65.0281 - val_rmse: 82.9137\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 7s 148ms/step - loss: 54.1942 - rmse: 69.3963 - val_loss: 63.0079 - val_rmse: 80.3059\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 7s 142ms/step - loss: 47.5213 - rmse: 61.6395 - val_loss: 73.7140 - val_rmse: 92.4781\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 44.1128 - rmse: 57.8033 - val_loss: 38.9809 - val_rmse: 51.3148\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 42.1492 - rmse: 56.1494 - val_loss: 35.5218 - val_rmse: 47.9326\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 38.3945 - rmse: 51.0992 - val_loss: 38.7120 - val_rmse: 50.0219\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 37.1177 - rmse: 49.1925 - val_loss: 35.5410 - val_rmse: 47.6037\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 34.7583 - rmse: 46.0078 - val_loss: 36.8625 - val_rmse: 50.1371\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 7s 145ms/step - loss: 32.5119 - rmse: 43.3241 - val_loss: 42.1773 - val_rmse: 56.6809\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 7s 142ms/step - loss: 31.6747 - rmse: 42.3044 - val_loss: 30.1153 - val_rmse: 40.0036\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 7s 147ms/step - loss: 30.2718 - rmse: 40.5328 - val_loss: 38.3143 - val_rmse: 51.6168\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 8s 150ms/step - loss: 28.4339 - rmse: 37.9689 - val_loss: 28.8535 - val_rmse: 37.9728\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 27.8612 - rmse: 37.2375 - val_loss: 31.8987 - val_rmse: 42.0581\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 27.1267 - rmse: 36.2890 - val_loss: 29.9191 - val_rmse: 39.1997\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 26.4234 - rmse: 35.2404 - val_loss: 30.4812 - val_rmse: 40.8416\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 25.2574 - rmse: 33.6988 - val_loss: 31.4149 - val_rmse: 41.2517\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 24.8586 - rmse: 33.0773 - val_loss: 27.4143 - val_rmse: 37.2083\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 24.1907 - rmse: 32.2680 - val_loss: 33.0352 - val_rmse: 44.2475\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 7s 144ms/step - loss: 23.2632 - rmse: 31.0944 - val_loss: 40.1008 - val_rmse: 52.0951\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 7s 146ms/step - loss: 22.8462 - rmse: 30.6592 - val_loss: 34.3695 - val_rmse: 45.2903\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 7s 142ms/step - loss: 22.4137 - rmse: 29.9039 - val_loss: 26.8354 - val_rmse: 36.0679\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 7s 147ms/step - loss: 21.8933 - rmse: 29.2903 - val_loss: 27.3247 - val_rmse: 37.8427\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 7s 143ms/step - loss: 21.4204 - rmse: 28.6018 - val_loss: 26.6727 - val_rmse: 35.9544\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 105.9256 - rmse: 129.5543 - val_loss: 58.4263 - val_rmse: 72.3611\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 83.9328 - rmse: 103.3471 - val_loss: 81.7820 - val_rmse: 100.7263\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 70.1296 - rmse: 88.4977 - val_loss: 55.5914 - val_rmse: 70.4467\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 56.8590 - rmse: 72.5437 - val_loss: 46.5094 - val_rmse: 61.4807\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 49.9229 - rmse: 64.6674 - val_loss: 47.2623 - val_rmse: 61.2524\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 46.6223 - rmse: 60.6114 - val_loss: 50.7834 - val_rmse: 62.5092\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 42.6311 - rmse: 56.0243 - val_loss: 42.0516 - val_rmse: 52.2453\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 40.3825 - rmse: 52.6928 - val_loss: 41.2608 - val_rmse: 51.4098\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 38.1209 - rmse: 49.8982 - val_loss: 49.8880 - val_rmse: 65.8605\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 36.4015 - rmse: 47.7521 - val_loss: 31.8975 - val_rmse: 43.2821\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 34.7147 - rmse: 45.6353 - val_loss: 35.5664 - val_rmse: 48.2054\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 32.1785 - rmse: 42.6209 - val_loss: 29.2802 - val_rmse: 38.7723\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 31.5597 - rmse: 41.6248 - val_loss: 28.4480 - val_rmse: 38.0040\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 29.7621 - rmse: 39.3357 - val_loss: 31.4948 - val_rmse: 41.5332\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 29.2892 - rmse: 38.8585 - val_loss: 26.9186 - val_rmse: 36.2248\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 27.8885 - rmse: 37.0651 - val_loss: 39.8689 - val_rmse: 52.1286\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 26.6220 - rmse: 35.4552 - val_loss: 34.3112 - val_rmse: 45.1608\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 25.8829 - rmse: 34.7576 - val_loss: 50.0952 - val_rmse: 64.2463\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 25.9518 - rmse: 34.8552 - val_loss: 39.5359 - val_rmse: 53.0152\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 24.9095 - rmse: 33.2424 - val_loss: 30.0954 - val_rmse: 41.1116\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 23.9887 - rmse: 32.0283 - val_loss: 39.0894 - val_rmse: 50.5998\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 23.3512 - rmse: 31.1145 - val_loss: 27.7713 - val_rmse: 37.4080\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 23.3377 - rmse: 30.8461 - val_loss: 33.1248 - val_rmse: 44.1196\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 21.7954 - rmse: 29.2459 - val_loss: 32.2783 - val_rmse: 43.1813\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 22.1451 - rmse: 29.4070 - val_loss: 25.0190 - val_rmse: 33.9848\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 13s 254ms/step - loss: 546.7414 - rmse: 679.5513 - val_loss: 84.6145 - val_rmse: 103.5584\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 83.0494 - rmse: 102.7964 - val_loss: 98.1563 - val_rmse: 125.9331\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 72.2247 - rmse: 89.5162 - val_loss: 65.7685 - val_rmse: 82.6355\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 72.7825 - rmse: 91.7084 - val_loss: 55.2155 - val_rmse: 72.0354\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 57.8765 - rmse: 73.6381 - val_loss: 66.6656 - val_rmse: 85.0404\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 50.3514 - rmse: 65.0660 - val_loss: 51.7812 - val_rmse: 64.6641\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 45.3629 - rmse: 58.8270 - val_loss: 50.3967 - val_rmse: 66.7497\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 9s 174ms/step - loss: 40.7189 - rmse: 53.6471 - val_loss: 37.8231 - val_rmse: 50.3139\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 39.3051 - rmse: 51.6870 - val_loss: 35.9921 - val_rmse: 48.2740\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 36.6765 - rmse: 48.2817 - val_loss: 36.5533 - val_rmse: 49.3842\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 34.5487 - rmse: 45.6061 - val_loss: 39.7072 - val_rmse: 54.0456\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 9s 183ms/step - loss: 32.6093 - rmse: 42.9417 - val_loss: 32.0568 - val_rmse: 43.1728\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 30.3641 - rmse: 40.4037 - val_loss: 39.1654 - val_rmse: 52.4048\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 29.5003 - rmse: 38.7512 - val_loss: 27.6500 - val_rmse: 36.8620\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 29.2568 - rmse: 38.4423 - val_loss: 29.0480 - val_rmse: 38.8145\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 9s 179ms/step - loss: 27.4860 - rmse: 36.2916 - val_loss: 33.5876 - val_rmse: 43.7857\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 26.0091 - rmse: 34.5442 - val_loss: 38.0887 - val_rmse: 49.5117\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 25.8590 - rmse: 34.4512 - val_loss: 41.2678 - val_rmse: 53.3026\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 25.0114 - rmse: 33.2267 - val_loss: 27.2678 - val_rmse: 36.6929\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 24.2843 - rmse: 31.9392 - val_loss: 28.2165 - val_rmse: 37.9783\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 23.5941 - rmse: 31.2713 - val_loss: 37.6198 - val_rmse: 50.0539\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 9s 180ms/step - loss: 22.9013 - rmse: 30.5353 - val_loss: 31.2789 - val_rmse: 40.7808\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 22.6318 - rmse: 29.9380 - val_loss: 30.1857 - val_rmse: 40.3107\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 21.7356 - rmse: 28.8890 - val_loss: 34.5749 - val_rmse: 44.7023\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 21.8998 - rmse: 28.9259 - val_loss: 25.4765 - val_rmse: 34.7693\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 354.2867 - rmse: 440.0732 - val_loss: 80.2161 - val_rmse: 99.1060\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 66.5615 - rmse: 83.4333 - val_loss: 97.9215 - val_rmse: 119.4608\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 66.3179 - rmse: 83.7293 - val_loss: 65.1480 - val_rmse: 79.3675\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 55.3041 - rmse: 70.7061 - val_loss: 49.5572 - val_rmse: 60.0175\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 48.1978 - rmse: 61.8513 - val_loss: 51.1029 - val_rmse: 67.3717\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 12s 230ms/step - loss: 43.9349 - rmse: 56.9630 - val_loss: 64.6732 - val_rmse: 77.5182\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 39.9956 - rmse: 52.4895 - val_loss: 44.8352 - val_rmse: 56.3996\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 11s 230ms/step - loss: 37.8677 - rmse: 49.3866 - val_loss: 33.3614 - val_rmse: 43.4604\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 35.5278 - rmse: 46.3837 - val_loss: 40.0066 - val_rmse: 52.9442\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 34.1358 - rmse: 44.7428 - val_loss: 32.0054 - val_rmse: 43.0025\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 31.5795 - rmse: 41.7549 - val_loss: 39.3801 - val_rmse: 52.5732\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 11s 230ms/step - loss: 30.4706 - rmse: 40.0803 - val_loss: 30.0661 - val_rmse: 40.6691\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 11s 227ms/step - loss: 29.9580 - rmse: 39.2849 - val_loss: 35.4239 - val_rmse: 47.0349\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 22s 448ms/step - loss: 27.7222 - rmse: 36.3577 - val_loss: 28.2600 - val_rmse: 37.9146\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 27.7631 - rmse: 36.2830 - val_loss: 30.1483 - val_rmse: 40.4881\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 26.2936 - rmse: 34.6827 - val_loss: 44.9301 - val_rmse: 59.3483\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 25.3370 - rmse: 33.5351 - val_loss: 49.7191 - val_rmse: 62.0155\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 24.4911 - rmse: 32.3313 - val_loss: 44.3709 - val_rmse: 57.5121\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 11s 226ms/step - loss: 23.3285 - rmse: 30.8841 - val_loss: 55.2891 - val_rmse: 71.4239\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 23.6870 - rmse: 31.1370 - val_loss: 28.8400 - val_rmse: 38.6123\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 23.1994 - rmse: 30.5663 - val_loss: 32.2646 - val_rmse: 43.4434\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 11s 226ms/step - loss: 21.9233 - rmse: 29.0062 - val_loss: 25.5103 - val_rmse: 34.8989\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 21.9845 - rmse: 28.9461 - val_loss: 28.3201 - val_rmse: 37.9220\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 11s 230ms/step - loss: 22.4767 - rmse: 29.8347 - val_loss: 26.4221 - val_rmse: 35.9558\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 21.5414 - rmse: 28.4192 - val_loss: 25.5976 - val_rmse: 34.6001\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 441.3401 - rmse: 545.6006 - val_loss: 73.9831 - val_rmse: 89.5622\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 70.9842 - rmse: 87.2132 - val_loss: 60.0345 - val_rmse: 75.1296\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 91.8505 - rmse: 115.0497 - val_loss: 80.1464 - val_rmse: 98.4356\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 84.2361 - rmse: 104.4991 - val_loss: 63.2997 - val_rmse: 81.0210\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 68.1148 - rmse: 86.7722 - val_loss: 52.1742 - val_rmse: 63.6375\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 58.3092 - rmse: 74.8466 - val_loss: 61.8144 - val_rmse: 80.6564\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 50.0565 - rmse: 64.9655 - val_loss: 51.8061 - val_rmse: 67.6378\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 11s 212ms/step - loss: 46.3983 - rmse: 60.0213 - val_loss: 35.3400 - val_rmse: 46.7379\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 11s 214ms/step - loss: 42.7809 - rmse: 55.7016 - val_loss: 37.9399 - val_rmse: 51.3525\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 39.5892 - rmse: 51.6683 - val_loss: 38.5935 - val_rmse: 52.2155\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 11s 211ms/step - loss: 37.3239 - rmse: 49.3382 - val_loss: 37.0583 - val_rmse: 49.8946\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 11s 219ms/step - loss: 34.2586 - rmse: 45.3266 - val_loss: 32.8525 - val_rmse: 44.3370\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 11s 219ms/step - loss: 37.2083 - rmse: 49.4670 - val_loss: 39.0206 - val_rmse: 51.9519\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 11s 211ms/step - loss: 32.3764 - rmse: 42.8821 - val_loss: 29.9657 - val_rmse: 40.4986\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 11s 223ms/step - loss: 31.1225 - rmse: 41.0775 - val_loss: 33.2027 - val_rmse: 42.3601\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 29.3071 - rmse: 38.8396 - val_loss: 40.5743 - val_rmse: 51.5346\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 11s 216ms/step - loss: 30.2957 - rmse: 40.2678 - val_loss: 34.1384 - val_rmse: 44.9038\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 27.5302 - rmse: 36.4409 - val_loss: 35.0975 - val_rmse: 46.6256\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 27.1814 - rmse: 35.9194 - val_loss: 36.6662 - val_rmse: 47.8362\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 11s 213ms/step - loss: 26.3836 - rmse: 34.7798 - val_loss: 30.7958 - val_rmse: 40.1558\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 11s 215ms/step - loss: 25.7342 - rmse: 33.9319 - val_loss: 34.7570 - val_rmse: 45.6241\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 11s 218ms/step - loss: 23.9548 - rmse: 31.6229 - val_loss: 32.8773 - val_rmse: 43.8630\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 24.2287 - rmse: 31.9764 - val_loss: 36.1596 - val_rmse: 46.3209\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 11s 224ms/step - loss: 24.0020 - rmse: 31.7247 - val_loss: 26.2223 - val_rmse: 35.7624\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 11s 219ms/step - loss: 23.1498 - rmse: 30.6924 - val_loss: 29.6467 - val_rmse: 39.8604\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 17s 330ms/step - loss: 665.5822 - rmse: 798.5524 - val_loss: 58.4911 - val_rmse: 72.4869\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 239ms/step - loss: 71.0981 - rmse: 88.6051 - val_loss: 93.1087 - val_rmse: 115.6791\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 11s 222ms/step - loss: 71.2461 - rmse: 89.4125 - val_loss: 59.5803 - val_rmse: 76.8142\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 11s 227ms/step - loss: 58.4284 - rmse: 74.4691 - val_loss: 44.0921 - val_rmse: 58.7270\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 50.3485 - rmse: 65.1437 - val_loss: 46.4313 - val_rmse: 59.0687\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 46.5747 - rmse: 60.5650 - val_loss: 39.2535 - val_rmse: 51.0694\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 242ms/step - loss: 41.5535 - rmse: 54.1894 - val_loss: 47.2238 - val_rmse: 59.2336\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 39.1040 - rmse: 51.0506 - val_loss: 38.5020 - val_rmse: 49.5077\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 12s 238ms/step - loss: 34.9424 - rmse: 45.8909 - val_loss: 49.8260 - val_rmse: 63.4490\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 33.5985 - rmse: 44.3639 - val_loss: 30.2560 - val_rmse: 41.1813\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 31.3040 - rmse: 41.2715 - val_loss: 30.7475 - val_rmse: 41.5049\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 30.2313 - rmse: 39.7642 - val_loss: 30.1438 - val_rmse: 39.7994\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 12s 250ms/step - loss: 29.3632 - rmse: 38.6632 - val_loss: 30.4848 - val_rmse: 41.0789\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 13s 268ms/step - loss: 27.7353 - rmse: 36.3493 - val_loss: 28.4701 - val_rmse: 38.1430\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 26.6674 - rmse: 35.3127 - val_loss: 33.7072 - val_rmse: 42.7818\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 25.4780 - rmse: 33.5808 - val_loss: 43.4272 - val_rmse: 56.1183\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 24.7580 - rmse: 32.9074 - val_loss: 40.7640 - val_rmse: 53.6609\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 23.8528 - rmse: 31.6960 - val_loss: 44.5281 - val_rmse: 56.9998\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 23.5826 - rmse: 31.2425 - val_loss: 27.0945 - val_rmse: 37.1146\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 11s 224ms/step - loss: 22.0840 - rmse: 29.4250 - val_loss: 28.4752 - val_rmse: 37.5142\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 22.5479 - rmse: 29.7760 - val_loss: 31.0066 - val_rmse: 41.3211\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 21.4760 - rmse: 28.5037 - val_loss: 27.2458 - val_rmse: 36.9604\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 20.6217 - rmse: 27.5468 - val_loss: 25.9820 - val_rmse: 34.7663\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 20.9708 - rmse: 27.8207 - val_loss: 25.0678 - val_rmse: 33.5414\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 19.9718 - rmse: 26.5095 - val_loss: 25.5440 - val_rmse: 35.0572\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 17s 347ms/step - loss: 2943.9047 - rmse: 3728.4500 - val_loss: 74.0702 - val_rmse: 91.3898\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 77.8636 - rmse: 95.5842 - val_loss: 88.2780 - val_rmse: 108.3418 rmse: 9\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 72.6787 - rmse: 89.8574 - val_loss: 56.7674 - val_rmse: 68.5864\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 67.5934 - rmse: 85.3315 - val_loss: 46.5106 - val_rmse: 61.1395\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 56.1052 - rmse: 72.8294 - val_loss: 75.8719 - val_rmse: 95.1822\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 49.0591 - rmse: 63.4846 - val_loss: 48.2705 - val_rmse: 59.3482\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 44.0190 - rmse: 57.4907 - val_loss: 39.9028 - val_rmse: 50.1793\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 40.9844 - rmse: 53.7500 - val_loss: 38.2651 - val_rmse: 49.0628\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 36.8131 - rmse: 48.5933 - val_loss: 36.2218 - val_rmse: 47.0225\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 35.8008 - rmse: 47.0522 - val_loss: 36.3152 - val_rmse: 49.2583\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 34.0116 - rmse: 44.7366 - val_loss: 34.6118 - val_rmse: 47.0689\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 12s 249ms/step - loss: 31.7638 - rmse: 41.9930 - val_loss: 27.2964 - val_rmse: 37.1945\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 12s 243ms/step - loss: 30.0205 - rmse: 39.7666 - val_loss: 32.1984 - val_rmse: 43.4320\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 13s 252ms/step - loss: 29.2174 - rmse: 38.6171 - val_loss: 36.9660 - val_rmse: 48.1662\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 27.7168 - rmse: 36.7805 - val_loss: 30.3870 - val_rmse: 40.2534\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 26.9800 - rmse: 35.5572 - val_loss: 34.1935 - val_rmse: 45.2662\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 25.8987 - rmse: 34.3711 - val_loss: 32.1788 - val_rmse: 42.8165\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 12s 246ms/step - loss: 25.9779 - rmse: 34.4059 - val_loss: 27.9181 - val_rmse: 37.7165\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 13s 251ms/step - loss: 23.7469 - rmse: 31.7370 - val_loss: 41.7109 - val_rmse: 54.4264\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 23.4926 - rmse: 31.3490 - val_loss: 27.5219 - val_rmse: 37.1965\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 23.4481 - rmse: 31.2733 - val_loss: 37.3492 - val_rmse: 49.4694\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 12s 240ms/step - loss: 22.7673 - rmse: 30.3341 - val_loss: 36.4622 - val_rmse: 47.7971\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 12s 244ms/step - loss: 21.9161 - rmse: 29.0382 - val_loss: 27.3004 - val_rmse: 37.6434\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 21.5268 - rmse: 28.7784 - val_loss: 25.9572 - val_rmse: 36.0106\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 12s 249ms/step - loss: 21.3310 - rmse: 28.6990 - val_loss: 25.3842 - val_rmse: 34.7502\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 20904.7947 - rmse: 24606.5723 - val_loss: 600.8712 - val_rmse: 797.6432\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 140.3774 - rmse: 192.2118 - val_loss: 74.6527 - val_rmse: 91.4095\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 92.3174 - rmse: 113.2106 - val_loss: 73.4103 - val_rmse: 91.6790\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 71.7455 - rmse: 87.9861 - val_loss: 66.6747 - val_rmse: 84.3350\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 15s 302ms/step - loss: 95.9315 - rmse: 119.2452 - val_loss: 65.7093 - val_rmse: 81.4714\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 66.4108 - rmse: 85.2731 - val_loss: 63.8337 - val_rmse: 80.1201\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 56.4337 - rmse: 71.0527 - val_loss: 47.1760 - val_rmse: 61.8537\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 15s 302ms/step - loss: 48.6786 - rmse: 62.7267 - val_loss: 43.6166 - val_rmse: 55.0281\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 44.3640 - rmse: 57.6226 - val_loss: 41.6591 - val_rmse: 53.0153\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 40.9178 - rmse: 53.2006 - val_loss: 34.8175 - val_rmse: 46.6401\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 38.2515 - rmse: 50.0916 - val_loss: 51.4229 - val_rmse: 66.8883\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 15s 302ms/step - loss: 35.9850 - rmse: 47.2799 - val_loss: 33.8200 - val_rmse: 43.5849\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 15s 299ms/step - loss: 34.7065 - rmse: 45.7615 - val_loss: 37.8984 - val_rmse: 50.6236\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 32.3323 - rmse: 42.8128 - val_loss: 31.7044 - val_rmse: 41.9307\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 30.3376 - rmse: 39.9741 - val_loss: 35.8113 - val_rmse: 47.1881\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 30.2973 - rmse: 40.2224 - val_loss: 30.6827 - val_rmse: 40.6323\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 28.8188 - rmse: 38.1762 - val_loss: 33.4181 - val_rmse: 43.9137\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 27.2651 - rmse: 36.3177 - val_loss: 37.9566 - val_rmse: 49.8179\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 27.0022 - rmse: 35.8827 - val_loss: 30.9933 - val_rmse: 40.5265\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 26.2080 - rmse: 34.8160 - val_loss: 34.7244 - val_rmse: 46.2027\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 24.4970 - rmse: 32.5952 - val_loss: 30.1856 - val_rmse: 40.1623\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 25.0982 - rmse: 33.4751 - val_loss: 29.2703 - val_rmse: 39.1154\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 23.9860 - rmse: 32.1248 - val_loss: 31.2846 - val_rmse: 42.1635\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 15s 306ms/step - loss: 24.2459 - rmse: 32.2661 - val_loss: 30.6708 - val_rmse: 40.7470\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 22.4006 - rmse: 29.9777 - val_loss: 30.0557 - val_rmse: 41.2705\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 20s 400ms/step - loss: 4354.4224 - rmse: 4913.5039 - val_loss: 110.1037 - val_rmse: 132.0227\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 88.2256 - rmse: 107.4808 - val_loss: 82.0454 - val_rmse: 99.8780\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 76.2603 - rmse: 94.0877 - val_loss: 68.4373 - val_rmse: 82.8311\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 15s 299ms/step - loss: 75.6038 - rmse: 94.0327 - val_loss: 75.5330 - val_rmse: 94.0159\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 61.9209 - rmse: 78.4007 - val_loss: 41.6025 - val_rmse: 53.3198\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 51.5647 - rmse: 66.4667 - val_loss: 51.8205 - val_rmse: 64.0532\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 45.5441 - rmse: 59.1848 - val_loss: 38.2956 - val_rmse: 49.7433\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 42.0372 - rmse: 54.8070 - val_loss: 37.3902 - val_rmse: 50.0584\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 39.4187 - rmse: 51.3471 - val_loss: 35.1977 - val_rmse: 47.3308\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 15s 302ms/step - loss: 37.2620 - rmse: 48.6553 - val_loss: 33.2764 - val_rmse: 44.9870\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 34.5421 - rmse: 45.4784 - val_loss: 38.4915 - val_rmse: 51.6245\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 33.1432 - rmse: 43.3141 - val_loss: 29.8790 - val_rmse: 40.9421\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 15s 291ms/step - loss: 30.7913 - rmse: 40.4445 - val_loss: 47.4339 - val_rmse: 62.4856\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 15s 299ms/step - loss: 31.0328 - rmse: 40.5717 - val_loss: 27.4962 - val_rmse: 37.9430\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 29.1580 - rmse: 38.0581 - val_loss: 28.5134 - val_rmse: 38.0072\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 28.3480 - rmse: 37.1964 - val_loss: 31.4423 - val_rmse: 41.0063\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 25.7691 - rmse: 34.2311 - val_loss: 51.3871 - val_rmse: 66.5318\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 26.0393 - rmse: 34.3019 - val_loss: 30.4723 - val_rmse: 40.1018\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 25.5305 - rmse: 33.7678 - val_loss: 38.8866 - val_rmse: 52.4298\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 23.7519 - rmse: 31.4493 - val_loss: 29.7627 - val_rmse: 39.8628\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 24.3108 - rmse: 32.1308 - val_loss: 34.9539 - val_rmse: 46.0968\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 29.2342 - rmse: 41.0762 - val_loss: 28.1889 - val_rmse: 38.8379\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 25.8667 - rmse: 34.7283 - val_loss: 39.8813 - val_rmse: 52.9469\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 24.3756 - rmse: 32.2418 - val_loss: 27.7926 - val_rmse: 37.6186\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 23.0246 - rmse: 30.5829 - val_loss: 26.5812 - val_rmse: 36.7664\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 22s 435ms/step - loss: 6093.5568 - rmse: 6733.1426 - val_loss: 68.2258 - val_rmse: 85.0697\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 71.4894 - rmse: 88.5746 - val_loss: 103.2850 - val_rmse: 125.0150\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 71.0281 - rmse: 88.5150 - val_loss: 89.1030 - val_rmse: 111.6595\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 68.2292 - rmse: 85.9961 - val_loss: 60.5531 - val_rmse: 75.8146\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 55.1591 - rmse: 70.8430 - val_loss: 42.6823 - val_rmse: 56.2250\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 48.4222 - rmse: 62.6442 - val_loss: 41.2325 - val_rmse: 52.7178\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 41.2650 - rmse: 54.2922 - val_loss: 45.7338 - val_rmse: 56.9413\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 38.0558 - rmse: 50.0985 - val_loss: 36.9838 - val_rmse: 49.7234\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 37.1063 - rmse: 48.7756 - val_loss: 32.9220 - val_rmse: 43.3656\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 35.6715 - rmse: 46.4591 - val_loss: 31.2999 - val_rmse: 41.5749\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 16s 310ms/step - loss: 32.8050 - rmse: 42.8928 - val_loss: 38.3538 - val_rmse: 51.8257\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 19s 377ms/step - loss: 30.4604 - rmse: 39.7872 - val_loss: 28.9828 - val_rmse: 39.6201\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 29.5724 - rmse: 38.7727 - val_loss: 33.6690 - val_rmse: 45.7784\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 27.5208 - rmse: 36.4197 - val_loss: 28.7848 - val_rmse: 38.8165\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 26.5712 - rmse: 34.8918 - val_loss: 30.8735 - val_rmse: 41.0297\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 25.7581 - rmse: 34.0148 - val_loss: 29.3058 - val_rmse: 38.9250\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 24.1251 - rmse: 32.0883 - val_loss: 28.2606 - val_rmse: 37.5918\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 23.4322 - rmse: 31.1345 - val_loss: 34.8609 - val_rmse: 46.6358\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 23.5383 - rmse: 31.2762 - val_loss: 45.6506 - val_rmse: 59.0821\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 17s 330ms/step - loss: 22.2325 - rmse: 29.6202 - val_loss: 28.1668 - val_rmse: 37.9948\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 21.5333 - rmse: 28.6213 - val_loss: 32.9953 - val_rmse: 44.6189\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 20.8632 - rmse: 27.6864 - val_loss: 30.2273 - val_rmse: 39.9752\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 20.8765 - rmse: 27.8998 - val_loss: 36.3973 - val_rmse: 47.7150\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 20.2753 - rmse: 26.9622 - val_loss: 27.5959 - val_rmse: 36.3425\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 19.5419 - rmse: 26.0289 - val_loss: 25.5204 - val_rmse: 34.7957\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 20s 396ms/step - loss: 11788.0841 - rmse: 12696.8652 - val_loss: 82.2730 - val_rmse: 97.0640\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 73.4206 - rmse: 88.8571 - val_loss: 65.1856 - val_rmse: 80.6132\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 75.9837 - rmse: 93.8261 - val_loss: 106.3295 - val_rmse: 128.3246\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 79.5466 - rmse: 98.8225 - val_loss: 69.3176 - val_rmse: 88.1153\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 15s 305ms/step - loss: 61.0838 - rmse: 78.1185 - val_loss: 71.9487 - val_rmse: 89.8320\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 48.9428 - rmse: 63.6480 - val_loss: 40.1981 - val_rmse: 53.6547\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 15s 302ms/step - loss: 44.3865 - rmse: 58.0222 - val_loss: 36.4570 - val_rmse: 48.9241\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 16s 310ms/step - loss: 40.6054 - rmse: 53.7187 - val_loss: 37.6794 - val_rmse: 49.1819\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 38.6507 - rmse: 50.7746 - val_loss: 33.7116 - val_rmse: 45.4245\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 15s 307ms/step - loss: 34.2195 - rmse: 45.4897 - val_loss: 30.3485 - val_rmse: 41.0100\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 15s 300ms/step - loss: 33.1772 - rmse: 43.8811 - val_loss: 43.5030 - val_rmse: 57.9827\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 15s 299ms/step - loss: 30.9235 - rmse: 40.8415 - val_loss: 29.7746 - val_rmse: 39.3813\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 28.7046 - rmse: 38.0115 - val_loss: 27.6674 - val_rmse: 37.1586\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 27.6114 - rmse: 36.4326 - val_loss: 27.0916 - val_rmse: 36.2555\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 26.6304 - rmse: 35.2254 - val_loss: 25.3407 - val_rmse: 34.9833\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 25.7007 - rmse: 34.0722 - val_loss: 39.2331 - val_rmse: 52.3265\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 20s 405ms/step - loss: 23.9095 - rmse: 31.9189 - val_loss: 33.4357 - val_rmse: 43.0925\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 23.5728 - rmse: 31.3156 - val_loss: 39.9974 - val_rmse: 53.7792\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 22.7839 - rmse: 30.4927 - val_loss: 28.9754 - val_rmse: 40.0641\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 27s 537ms/step - loss: 22.4391 - rmse: 30.1648 - val_loss: 28.8913 - val_rmse: 39.2937\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 20s 391ms/step - loss: 21.6583 - rmse: 28.7559 - val_loss: 28.4797 - val_rmse: 39.0683\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 20.9949 - rmse: 28.0071 - val_loss: 33.3095 - val_rmse: 45.3753\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 21.4140 - rmse: 28.4461 - val_loss: 27.6345 - val_rmse: 37.7035\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 20.1360 - rmse: 27.0150 - val_loss: 24.8148 - val_rmse: 33.7366\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 20.1500 - rmse: 26.9738 - val_loss: 25.0570 - val_rmse: 33.9996\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 22s 449ms/step - loss: 15828.4659 - rmse: 17586.9707 - val_loss: 68.1885 - val_rmse: 82.8108\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 19s 376ms/step - loss: 65.3459 - rmse: 81.2062 - val_loss: 56.2339 - val_rmse: 68.1750\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 66.7744 - rmse: 84.7505 - val_loss: 81.1163 - val_rmse: 103.2516\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 19s 370ms/step - loss: 83.3506 - rmse: 104.8865 - val_loss: 56.4304 - val_rmse: 69.6523\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 19s 370ms/step - loss: 56.7983 - rmse: 72.5729 - val_loss: 58.3507 - val_rmse: 71.8054\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 48.2182 - rmse: 61.9921 - val_loss: 38.4921 - val_rmse: 49.5270\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 42.3336 - rmse: 55.0409 - val_loss: 47.1089 - val_rmse: 61.3098\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 18s 368ms/step - loss: 36.8760 - rmse: 48.2616 - val_loss: 32.2369 - val_rmse: 42.7871\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 19s 370ms/step - loss: 34.9604 - rmse: 45.6606 - val_loss: 32.1366 - val_rmse: 42.2962\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 32.7493 - rmse: 42.8065 - val_loss: 30.3734 - val_rmse: 41.6274\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 30.5764 - rmse: 40.2034 - val_loss: 39.0047 - val_rmse: 52.1446\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 28.6237 - rmse: 37.6905 - val_loss: 28.1468 - val_rmse: 38.0372\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 18s 367ms/step - loss: 27.7968 - rmse: 36.9335 - val_loss: 30.8551 - val_rmse: 41.6841\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 18s 365ms/step - loss: 26.0555 - rmse: 34.1360 - val_loss: 26.4742 - val_rmse: 35.9104\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 19s 376ms/step - loss: 25.2177 - rmse: 33.1940 - val_loss: 29.1495 - val_rmse: 39.2933\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 24.6645 - rmse: 32.4243 - val_loss: 28.9123 - val_rmse: 38.9682\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 23.4849 - rmse: 31.0982 - val_loss: 41.5958 - val_rmse: 54.7969\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 18s 369ms/step - loss: 23.2209 - rmse: 30.7236 - val_loss: 40.2675 - val_rmse: 52.7602\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 21.9253 - rmse: 29.0693 - val_loss: 43.5819 - val_rmse: 56.7793\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 21.4592 - rmse: 28.5559 - val_loss: 25.9694 - val_rmse: 35.5957\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 20.4365 - rmse: 27.2047 - val_loss: 33.1899 - val_rmse: 44.2768\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 19s 372ms/step - loss: 20.7209 - rmse: 27.5531 - val_loss: 26.1828 - val_rmse: 34.9155\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 18s 361ms/step - loss: 20.1071 - rmse: 26.8287 - val_loss: 27.1403 - val_rmse: 36.2987\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 18s 362ms/step - loss: 19.9776 - rmse: 26.5903 - val_loss: 25.5237 - val_rmse: 34.3024\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 18s 368ms/step - loss: 19.1216 - rmse: 25.5569 - val_loss: 24.7076 - val_rmse: 34.1867\n",
      "Train for 50 steps, validate for 50 steps\n",
      "Epoch 1/25\n",
      "50/50 [==============================] - 25s 490ms/step - loss: 206.6853 - rmse: 257.0735 - val_loss: 63.1239 - val_rmse: 81.5932\n",
      "Epoch 2/25\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 66.2887 - rmse: 83.4030 - val_loss: 53.4032 - val_rmse: 64.9720\n",
      "Epoch 3/25\n",
      "50/50 [==============================] - 20s 390ms/step - loss: 65.8014 - rmse: 83.1541 - val_loss: 47.2090 - val_rmse: 60.6960\n",
      "Epoch 4/25\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 54.3528 - rmse: 70.0835 - val_loss: 37.9085 - val_rmse: 50.0255\n",
      "Epoch 5/25\n",
      "50/50 [==============================] - 20s 395ms/step - loss: 48.6938 - rmse: 63.6517 - val_loss: 39.8724 - val_rmse: 52.3073\n",
      "Epoch 6/25\n",
      "50/50 [==============================] - 19s 386ms/step - loss: 42.7049 - rmse: 55.7399 - val_loss: 39.8186 - val_rmse: 50.6056\n",
      "Epoch 7/25\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 39.0796 - rmse: 51.3360 - val_loss: 45.7988 - val_rmse: 58.5321\n",
      "Epoch 8/25\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 36.6128 - rmse: 48.1996 - val_loss: 32.3080 - val_rmse: 43.0405\n",
      "Epoch 9/25\n",
      "50/50 [==============================] - 19s 386ms/step - loss: 33.5393 - rmse: 44.3622 - val_loss: 40.6628 - val_rmse: 53.0719\n",
      "Epoch 10/25\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 32.1319 - rmse: 42.3958 - val_loss: 29.8939 - val_rmse: 40.2264\n",
      "Epoch 11/25\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 30.3862 - rmse: 40.1834 - val_loss: 35.7339 - val_rmse: 47.9628\n",
      "Epoch 12/25\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 29.5771 - rmse: 38.9718 - val_loss: 28.6713 - val_rmse: 38.8422\n",
      "Epoch 13/25\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 28.2497 - rmse: 37.4823 - val_loss: 32.5074 - val_rmse: 43.7690\n",
      "Epoch 14/25\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 27.3258 - rmse: 36.1211 - val_loss: 29.4944 - val_rmse: 40.0779\n",
      "Epoch 15/25\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 25.5811 - rmse: 33.9830 - val_loss: 26.6484 - val_rmse: 36.2126\n",
      "Epoch 16/25\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 24.5925 - rmse: 32.7696 - val_loss: 25.9185 - val_rmse: 35.1643\n",
      "Epoch 17/25\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 24.0699 - rmse: 32.0987 - val_loss: 31.0715 - val_rmse: 41.7858\n",
      "Epoch 18/25\n",
      "50/50 [==============================] - 20s 405ms/step - loss: 23.4352 - rmse: 31.1970 - val_loss: 41.9997 - val_rmse: 54.2203\n",
      "Epoch 19/25\n",
      "50/50 [==============================] - 19s 390ms/step - loss: 23.0376 - rmse: 30.7423 - val_loss: 35.8580 - val_rmse: 48.6024\n",
      "Epoch 20/25\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 22.2707 - rmse: 29.8002 - val_loss: 28.7386 - val_rmse: 38.6083\n",
      "Epoch 21/25\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 21.6722 - rmse: 29.0167 - val_loss: 27.0607 - val_rmse: 36.5541\n",
      "Epoch 22/25\n",
      "50/50 [==============================] - 19s 386ms/step - loss: 21.9101 - rmse: 29.1768 - val_loss: 28.0495 - val_rmse: 37.0560\n",
      "Epoch 23/25\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 21.1772 - rmse: 28.2462 - val_loss: 33.8586 - val_rmse: 45.1822\n",
      "Epoch 24/25\n",
      "50/50 [==============================] - 20s 395ms/step - loss: 20.3268 - rmse: 27.2234 - val_loss: 25.5124 - val_rmse: 35.6626\n",
      "Epoch 25/25\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 20.9178 - rmse: 27.9006 - val_loss: 25.6646 - val_rmse: 35.3851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_dict = {\"epochs\":[25], \"layer_count\":[2, 3], \"node_number\":[120, 140, 160, 180, 200, 220, 240], \"dropout\":[0.3, 0.7]}\n",
    "\n",
    "grid_search.search(feature_dict, c2g_data, windows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>layer_count</th>\n",
       "      <th>node_number</th>\n",
       "      <th>dropout</th>\n",
       "      <th>loss</th>\n",
       "      <th>rmse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>220</td>\n",
       "      <td>0.3</td>\n",
       "      <td>18.508599</td>\n",
       "      <td>24.894522</td>\n",
       "      <td>22.956626</td>\n",
       "      <td>31.818382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>240</td>\n",
       "      <td>0.3</td>\n",
       "      <td>18.981148</td>\n",
       "      <td>25.556908</td>\n",
       "      <td>24.707572</td>\n",
       "      <td>34.186726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20.642672</td>\n",
       "      <td>27.936657</td>\n",
       "      <td>24.743443</td>\n",
       "      <td>34.534412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.177676</td>\n",
       "      <td>27.377291</td>\n",
       "      <td>24.779557</td>\n",
       "      <td>33.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.777988</td>\n",
       "      <td>29.130457</td>\n",
       "      <td>24.861382</td>\n",
       "      <td>34.680988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.008420</td>\n",
       "      <td>29.407026</td>\n",
       "      <td>25.019000</td>\n",
       "      <td>33.984753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>220</td>\n",
       "      <td>0.7</td>\n",
       "      <td>19.742950</td>\n",
       "      <td>26.973797</td>\n",
       "      <td>25.056960</td>\n",
       "      <td>33.999569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.165813</td>\n",
       "      <td>28.698973</td>\n",
       "      <td>25.384185</td>\n",
       "      <td>34.750221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.585495</td>\n",
       "      <td>28.925949</td>\n",
       "      <td>25.476516</td>\n",
       "      <td>34.769283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>220</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.414039</td>\n",
       "      <td>26.028904</td>\n",
       "      <td>25.520435</td>\n",
       "      <td>34.795708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.812252</td>\n",
       "      <td>26.509514</td>\n",
       "      <td>25.543987</td>\n",
       "      <td>35.057205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.399918</td>\n",
       "      <td>27.350542</td>\n",
       "      <td>25.570821</td>\n",
       "      <td>34.408104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.226410</td>\n",
       "      <td>28.419163</td>\n",
       "      <td>25.597617</td>\n",
       "      <td>34.600052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.676004</td>\n",
       "      <td>29.009121</td>\n",
       "      <td>25.632041</td>\n",
       "      <td>35.592152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>240</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20.766723</td>\n",
       "      <td>27.900637</td>\n",
       "      <td>25.664571</td>\n",
       "      <td>35.385147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.7</td>\n",
       "      <td>22.642993</td>\n",
       "      <td>30.188459</td>\n",
       "      <td>25.680691</td>\n",
       "      <td>35.301163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>220</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.210831</td>\n",
       "      <td>28.550320</td>\n",
       "      <td>26.489916</td>\n",
       "      <td>35.748020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7</td>\n",
       "      <td>22.740067</td>\n",
       "      <td>30.582888</td>\n",
       "      <td>26.581236</td>\n",
       "      <td>36.766434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.219718</td>\n",
       "      <td>28.601770</td>\n",
       "      <td>26.672659</td>\n",
       "      <td>35.954418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.074897</td>\n",
       "      <td>28.308435</td>\n",
       "      <td>26.873160</td>\n",
       "      <td>37.189133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.649056</td>\n",
       "      <td>29.076790</td>\n",
       "      <td>27.050006</td>\n",
       "      <td>36.850803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.802521</td>\n",
       "      <td>27.824192</td>\n",
       "      <td>27.205074</td>\n",
       "      <td>36.972336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.767445</td>\n",
       "      <td>27.956345</td>\n",
       "      <td>27.348841</td>\n",
       "      <td>37.833447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.054404</td>\n",
       "      <td>25.546606</td>\n",
       "      <td>27.841339</td>\n",
       "      <td>38.364098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.393919</td>\n",
       "      <td>28.820768</td>\n",
       "      <td>28.050756</td>\n",
       "      <td>38.874641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7</td>\n",
       "      <td>23.258836</td>\n",
       "      <td>31.035557</td>\n",
       "      <td>29.158530</td>\n",
       "      <td>39.635262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>0.7</td>\n",
       "      <td>23.004388</td>\n",
       "      <td>30.692368</td>\n",
       "      <td>29.646716</td>\n",
       "      <td>39.860443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.144098</td>\n",
       "      <td>29.977722</td>\n",
       "      <td>30.055713</td>\n",
       "      <td>41.270470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  layer_count  node_number  dropout       loss       rmse  \\\n",
       "0       25            2          220      0.3  18.508599  24.894522   \n",
       "1       25            3          240      0.3  18.981148  25.556908   \n",
       "2       25            2          240      0.7  20.642672  27.936657   \n",
       "3       25            2          160      0.3  20.177676  27.377291   \n",
       "4       25            2          180      0.7  21.777988  29.130457   \n",
       "5       25            3          140      0.3  22.008420  29.407026   \n",
       "6       25            3          220      0.7  19.742950  26.973797   \n",
       "7       25            3          180      0.7  21.165813  28.698973   \n",
       "8       25            3          140      0.7  21.585495  28.925949   \n",
       "9       25            3          220      0.3  19.414039  26.028904   \n",
       "10      25            3          180      0.3  19.812252  26.509514   \n",
       "11      25            2          180      0.3  20.399918  27.350542   \n",
       "12      25            3          160      0.3  21.226410  28.419163   \n",
       "13      25            2          140      0.7  21.676004  29.009121   \n",
       "14      25            3          240      0.7  20.766723  27.900637   \n",
       "15      25            2          120      0.7  22.642993  30.188459   \n",
       "16      25            2          220      0.7  21.210831  28.550320   \n",
       "17      25            3          200      0.7  22.740067  30.582888   \n",
       "18      25            3          120      0.7  21.219718  28.601770   \n",
       "19      25            3          120      0.3  21.074897  28.308435   \n",
       "20      25            2          120      0.3  21.649056  29.076790   \n",
       "21      25            2          140      0.3  20.802521  27.824192   \n",
       "22      25            2          200      0.3  20.767445  27.956345   \n",
       "23      25            2          240      0.3  19.054404  25.546606   \n",
       "24      25            2          160      0.7  21.393919  28.820768   \n",
       "25      25            2          200      0.7  23.258836  31.035557   \n",
       "26      25            3          160      0.7  23.004388  30.692368   \n",
       "27      25            3          200      0.3  22.144098  29.977722   \n",
       "\n",
       "     val_loss   val_rmse  \n",
       "0   22.956626  31.818382  \n",
       "1   24.707572  34.186726  \n",
       "2   24.743443  34.534412  \n",
       "3   24.779557  33.905300  \n",
       "4   24.861382  34.680988  \n",
       "5   25.019000  33.984753  \n",
       "6   25.056960  33.999569  \n",
       "7   25.384185  34.750221  \n",
       "8   25.476516  34.769283  \n",
       "9   25.520435  34.795708  \n",
       "10  25.543987  35.057205  \n",
       "11  25.570821  34.408104  \n",
       "12  25.597617  34.600052  \n",
       "13  25.632041  35.592152  \n",
       "14  25.664571  35.385147  \n",
       "15  25.680691  35.301163  \n",
       "16  26.489916  35.748020  \n",
       "17  26.581236  36.766434  \n",
       "18  26.672659  35.954418  \n",
       "19  26.873160  37.189133  \n",
       "20  27.050006  36.850803  \n",
       "21  27.205074  36.972336  \n",
       "22  27.348841  37.833447  \n",
       "23  27.841339  38.364098  \n",
       "24  28.050756  38.874641  \n",
       "25  29.158530  39.635262  \n",
       "26  29.646716  39.860443  \n",
       "27  30.055713  41.270470  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.evaluations.to_csv(f'results\\\\GridSearch_Results\\\\c2g_grid_search_{datetime.now().strftime(\"%m-%d-%Y_%Hh%Mmin%Ss\")}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('CarSharingEnv': conda)",
   "language": "python",
   "name": "python37664bitcarsharingenvcondaf61cd503e3274ce6bd69d58b01a97e08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
