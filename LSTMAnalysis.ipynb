{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evo_cleaning(data):\n",
    "    # only travels with more than 30 minutes of duration, that is the limit of cancellation of a reserve\n",
    "    data = data.loc[(data['duration'] > 30) | (data['distance'] > 3)]\n",
    "\n",
    "    data.Start_time = pd.to_datetime(data.Start_time)\n",
    "    data.End_time = pd.to_datetime(data.End_time)\n",
    "\n",
    "    # Colleting vehicle ids\n",
    "    car_ids = list(data.Id.unique())\n",
    "\n",
    "    # Removing uncommon ids\n",
    "    # Ex: 4c5865a3-4b03-40f6-a3a8-d4e94aae3b17\n",
    "    ids_uncommon = [id for id in car_ids if id.find('-') != -1]\n",
    "    car_ids = [id for id in car_ids if id.find('-') == -1]\n",
    "\n",
    "    data = data.loc[~data.Id.isin(ids_uncommon)]\n",
    "    \n",
    "    # Removing microseconds from the dates\n",
    "    data.Start_time = data.Start_time.apply(lambda x: x.replace(microsecond=0))\n",
    "    data.End_time = data.End_time.apply(lambda x: x.replace(microsecond=0))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "evo_data = pd.read_csv('data/evo_travels.csv')\n",
    "modo_data = pd.read_csv('data/modo_travels.csv')\n",
    "c2g_data = pd.read_csv('data/car2go_travels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning process\n",
    "evo_data = evo_cleaning(evo_data)\n",
    "\n",
    "modo_data.start = modo_data.start.apply(lambda x: datetime.fromtimestamp(x))\n",
    "modo_data.end = modo_data.end.apply(lambda x: datetime.fromtimestamp(x))\n",
    "\n",
    "c2g_data.init_time = c2g_data.init_time.apply(lambda x: datetime.fromtimestamp(x))\n",
    "c2g_data.final_time = c2g_data.final_time.apply(lambda x: datetime.fromtimestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_period = '05-25'\n",
    "end_period = '06-15'\n",
    "\n",
    "evo_data = evo_data[(evo_data.Start_time >= '2018-'+init_period) & (evo_data.End_time <= '2018-'+end_period)]\n",
    "modo_data = modo_data[(modo_data.start >= '2018-'+init_period) & (modo_data.end <= '2018-'+end_period)]\n",
    "c2g_data = c2g_data[(c2g_data.init_time >= '2017-'+init_period) & (c2g_data.final_time <= '2017-'+end_period)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    \"\"\"\n",
    "    Reshape the data to usual representation of [batch size, history size, dimensionality]\n",
    "    batch size - length of the data\n",
    "    history size - size of the past window of information.\n",
    "    dimensionality - number of variables\n",
    "    \n",
    "    target_size - Is how far in the future does the model need to learn to predict.\n",
    "    The target_size is the label that needs to be predicted\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        # Reshape data from (history_size,) to (history_size, 1)\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_size(data, size=0.7):\n",
    "    # 70% of the data to use as train set\n",
    "    train_split = int(len(data) * size)\n",
    "    return train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(data):\n",
    "    values = data.values\n",
    "    norm_data = (values - values.min())/(values.max() - values.min())\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis the aim is to predict the number of travels using a univariate LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organization of the data for the model\n",
    "evo_travel_count = evo_data.Start_time.value_counts(sort=False)\n",
    "modo_travel_count = modo_data.start.value_counts(sort=False)\n",
    "c2g_travel_count = c2g_data.init_time.value_counts(sort=False)\n",
    "\n",
    "evo_norm = norm_data(evo_travel_count)\n",
    "modo_norm = norm_data(modo_travel_count)\n",
    "c2g_norm = norm_data(c2g_travel_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(data, train_split, history_length=60, future_target=0, batch_size=256, buffer_size=10000):\n",
    "    \n",
    "    univariate_past_history = history_length\n",
    "    univariate_future_target = future_target\n",
    "\n",
    "    x_train_uni, y_train_uni = univariate_data(data, 0, train_split,\n",
    "                                               univariate_past_history,\n",
    "                                               univariate_future_target)\n",
    "    x_val_uni, y_val_uni = univariate_data(data, train_split, None,\n",
    "                                           univariate_past_history,\n",
    "                                           univariate_future_target)\n",
    "\n",
    "    train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "    train_univariate = train_univariate.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
    "\n",
    "    val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "    val_univariate = val_univariate.batch(batch_size).repeat()\n",
    "    \n",
    "    shape = x_train_uni.shape[-2:]\n",
    "    \n",
    "    return train_univariate, val_univariate, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(train_data, validate_data, shape, epochs=10, evaluation_interval=200):\n",
    "    simple_lstm_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(8, input_shape=shape),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    simple_lstm_model.fit(train_data, epochs=epochs,\n",
    "                      steps_per_epoch=evaluation_interval,\n",
    "                      validation_data=validate_data, validation_steps=50)\n",
    "    \n",
    "    return simple_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data on train and validate sets\n",
    "evo_train, evo_val, evo_shape = train_val_split(data=evo_norm, train_split=train_size(evo_norm))\n",
    "modo_train, modo_val, modo_shape = train_val_split(data=modo_norm, train_split=train_size(modo_norm))\n",
    "c2g_train, c2g_val, c2g_shape = train_val_split(data=c2g_norm, train_split=train_size(c2g_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evo Model\n",
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 28s 138ms/step - loss: 0.0133 - val_loss: 0.0129\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.0130 - val_loss: 0.0130\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Modo Model\n",
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 0.0561 - val_loss: 0.0480\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 11s 57ms/step - loss: 0.0533 - val_loss: 0.0473\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 0.0531 - val_loss: 0.0472\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 0.0531 - val_loss: 0.0471\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.0531 - val_loss: 0.0470\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 0.0529 - val_loss: 0.0470\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 13s 63ms/step - loss: 0.0531 - val_loss: 0.0470\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 13s 63ms/step - loss: 0.0531 - val_loss: 0.0469\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 13s 63ms/step - loss: 0.0530 - val_loss: 0.0469\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 13s 63ms/step - loss: 0.0530 - val_loss: 0.0469\n",
      "Car2Go Model\n",
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 14s 68ms/step - loss: 0.0978 - val_loss: 0.0986\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 14s 69ms/step - loss: 0.0977 - val_loss: 0.0986\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 15s 75ms/step - loss: 0.0974 - val_loss: 0.0988\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 0.0978 - val_loss: 0.0985\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 0.0978 - val_loss: 0.0985\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 0.0976 - val_loss: 0.0987\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 0.0981 - val_loss: 0.0986\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.0977 - val_loss: 0.0985\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 0.0973 - val_loss: 0.0985\n"
     ]
    }
   ],
   "source": [
    "# generating the model for each dataset\n",
    "print('Evo Model')\n",
    "evo_model = lstm_model(evo_train, evo_val, evo_shape)\n",
    "print('Modo Model')\n",
    "modo_model = lstm_model(modo_train, modo_val, modo_shape)\n",
    "print('Car2Go Model')\n",
    "c2g_model = lstm_model(c2g_train, c2g_val, c2g_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evo_val.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01234568]\n",
      " [0.02314815]\n",
      " [0.04475309]\n",
      " [0.02623457]\n",
      " [0.        ]\n",
      " [0.02160494]\n",
      " [0.03395062]\n",
      " [0.04012346]\n",
      " [0.03395062]\n",
      " [0.02314815]\n",
      " [0.02932099]\n",
      " [0.02314815]\n",
      " [0.00925926]\n",
      " [0.0617284 ]\n",
      " [0.03703704]\n",
      " [0.02623457]\n",
      " [0.04012346]\n",
      " [0.02469136]\n",
      " [0.01080247]\n",
      " [0.00925926]\n",
      " [0.02314815]\n",
      " [0.01851852]\n",
      " [0.        ]\n",
      " [0.00617284]\n",
      " [0.02932099]\n",
      " [0.05864198]\n",
      " [0.02160494]\n",
      " [0.02469136]\n",
      " [0.02160494]\n",
      " [0.02160494]\n",
      " [0.02777778]\n",
      " [0.02777778]\n",
      " [0.03240741]\n",
      " [0.00925926]\n",
      " [0.00308642]\n",
      " [0.04475309]\n",
      " [0.        ]\n",
      " [0.02006173]\n",
      " [0.0462963 ]\n",
      " [0.02623457]\n",
      " [0.03858025]\n",
      " [0.03549383]\n",
      " [0.04783951]\n",
      " [0.0308642 ]\n",
      " [0.03240741]\n",
      " [0.02469136]\n",
      " [0.04166667]\n",
      " [0.0308642 ]\n",
      " [0.02469136]\n",
      " [0.00462963]\n",
      " [0.01388889]\n",
      " [0.02623457]\n",
      " [0.00154321]\n",
      " [0.02160494]\n",
      " [0.        ]\n",
      " [0.00154321]\n",
      " [0.00462963]\n",
      " [0.00308642]\n",
      " [0.06790123]\n",
      " [0.02777778]] 0.007716049382716049\n"
     ]
    }
   ],
   "source": [
    "for x, y in evo_val.take(1):\n",
    "    print(x[0].numpy(), y[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
